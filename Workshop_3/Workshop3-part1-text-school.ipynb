{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95c92a6-3778-470e-acd5-1dbb53e7d0c8",
   "metadata": {},
   "source": [
    "# Part 1 - Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790039b-3ad1-4939-87d0-fd30a6fc8e76",
   "metadata": {},
   "source": [
    "In this workshop, we will provide an introduction to working with the various types of hydrological data you are likely to encounter.\n",
    "\n",
    "The learning objectives are to:\n",
    "\n",
    "- Learn how to load various types of data from object storage (\"the cloud\")\n",
    "- Learn about the different types of textual file formats that are commonly used and how to work with them in Python using Pandas and Numpy\n",
    "- Learn about the different types of spatial file formats that are commonly used and how to work with them in Python using GeoPandas\n",
    "- Learn about the new Zarr format for multi-dimensional large datasets and how to work with it in Python using Xarray\n",
    "- Learn how to utilise Matplotlib and Cartopy's plotting libraries to quickly visualise data\n",
    "\n",
    "We will be teaching via Jupyter notebooks - which you are hopefully reading this from. We will be going through the notebooks cell-by-cell together. There will be some code cells that you can just run (use ```Shift+Enter```) to run a given cell only, some where you will code along with me, and some Exercises for you to complete on your own. \n",
    "\n",
    "There will be three notebooks covering, respectively, textual data, vector data, gridded or multi-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c3b1e-c58d-4fcd-a2df-9ef5c8d7ec3b",
   "metadata": {},
   "source": [
    "In this, first, notebook we will focus on textual, human-readable data.\n",
    "\n",
    "This notebook was designed for a session as part of the UKCEH Summer School. It does not cover all aspects of textual data use by the Python scientific communities. Additional resources can be found throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f709bf0-565b-421e-a40d-92af1cb314e9",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- CSV data\n",
    "    - Simple exploring and plotting data with Pandas\n",
    "    - Handling missing data\n",
    "    - Exploring the powerful time-handling of Pandas\n",
    "- ASCII Grid data\n",
    "    - Reading text data with pure Python\n",
    "    - Loading Grid data into Xarray\n",
    "    - Basic plotting with Xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500a3bc-2336-49bc-8d34-fed9cfdcec40",
   "metadata": {},
   "source": [
    "Load required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39db0c2-5680-465b-8ead-43164a0e7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e382d-574a-4b7e-95b7-54b1c6691501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e0df6-8643-4768-a80b-86e65c0744f3",
   "metadata": {},
   "source": [
    "We will be using data stored in an object storage bucket through out the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b270315-c5a4-4b49-ba10-9c127ed8bd77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True, client_kwargs={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\"})\n",
    "s3.ls('s3://example-data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939219e-d698-4091-b6c2-7d11d38bf658",
   "metadata": {},
   "source": [
    "## Textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9286e5-6ba0-4860-9be3-2b9a5408c591",
   "metadata": {},
   "source": [
    "Often referred to as '[ASCII](https://en.wikipedia.org/wiki/ASCII)', textual data is data that is human-readable, i.e. you can open it with a text editor and see the actual data written down. This differs from binary data formats - like NetCDF, Zarr, Shapefiles - which are stored directly as specifically laid-out binary digits - 0s and 1s - which only computers can read directly. Exactly how the 0s and 1s are laid out and exactly how they represent/translate to the data we are interested in (the encoding) defines the file format. They need special programmes that understand the layout and encoding in order to be read in and processed. \n",
    "\n",
    "But enough about binary data for now, we're here to get to grips with textual data first. Arguably it is easier to work with as all you need is a text editor to get started. Data that is typically stored as ASCII includes gridded outputs from some hydrological models and rain-gauge data.\n",
    "\n",
    "We will be using [Pandas](https://pandas.pydata.org/), a powerful python library for working with tabulated data, which is what you usually get in textual data files. We will explore some of what you can do with Pandas here, but there is a huge amount of functionality we won't cover that is documented on the Pandas website. \n",
    "\n",
    "We will also be reading in data that is stored remotely on object storage instead of locally on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973f120-4622-4f75-a7a1-56465ea16bc4",
   "metadata": {},
   "source": [
    "Let's start with some simple textual river flow data, such as that which might be output by a hydrological model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c403fc-0ed7-46ff-88aa-c4578f14ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdata = pd.read_csv('s3://example-data/obsflows.csv', storage_options={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\", 'anon': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b80ea-aa2c-4c58-994d-e19f6f4598eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8a016-9ca7-4375-b564-2eb1dfb00d4f",
   "metadata": {},
   "source": [
    "This is **csv** data. CSV stands for \"Comma Separated Values\" and means that if you were to open this data up in a text editor you would see a bunch of values separated by commas. In this case the commas are the \"delimeters\" that separate or \"delimit\" the individual values from each other. Even though CSV stands for **Comma** Separated Values, you may also see other characters being used as delimiters in CSV files, the most common alternative is tabs, which show up as gaps between the numbers in a text editor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c87b8-d5c8-40f6-a4bd-0f5405a497f0",
   "metadata": {},
   "source": [
    "Taking a look at the data table now, we can see that we have rows representing the date and columns representing... well it's not actually clear is it. The column headers are in fact catchment IDs, a 5-digit number given to all catchments in the UK, with the first two numbers representing the river-basin the catchment ultimately drains into and the last 3 numbers are zero-padded and are representing the sub-catchment within this river basin. So 39001 translates as 'river basin number 39' (which is the Thames) and sub-catchment number '01' (which in this case is the Thames as far as Kingston). More information on how catchments are identified here: https://nrfa.ceh.ac.uk/data/about-data/catchment-information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74980f5-d450-4ac3-a43f-2bd712af8362",
   "metadata": {},
   "source": [
    "This is flow data, so each value in the table will be a flow amount in $m^3/day$. We can see straight away that there are a lot of -1 values. This is likely missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb28376-5eb8-423b-8dc6-fd105db888ac",
   "metadata": {},
   "source": [
    "We can also see that the date information is split over three columns (day, month, and year). Pandas has some excellent date-time handling capabilities, but it sometimes needs to be told how to interpret the date-time information in the data first before this can be unlocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a33e4-cc8d-4428-a110-05c88939ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "obsdata = pd.read_csv('s3://example-data/obsflows.csv', \n",
    "                      # FILL IN THIS LINE \n",
    "                      storage_options={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\", 'anon': True})\n",
    "# SET THE NEW INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f787b-0d56-4815-9ae2-d812935e4dcc",
   "metadata": {},
   "source": [
    "The first line is where we read in the data, note we have now added in an extra argument telling pandas how to handle the date-time information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4deff92-7e0a-41bd-af31-f81be045bdb8",
   "metadata": {},
   "source": [
    "The second line sets the new 'Times' column that we have created from the original 'day', 'month', and 'year' columns as the Index, which is what Pandas uses to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1550e-8eae-4297-9338-3405c7e20858",
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824305b-08c1-4fab-8183-a56b57f697c6",
   "metadata": {},
   "source": [
    "Let's now have a look at one of the catchments in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3a223-b95e-4ef4-8ce7-1e10144b4da0",
   "metadata": {},
   "source": [
    "We can cleverly select out the date range and catchment we wish to plot using the row and column names. As the row names (or the 'Index' in Pandas parlance) contains labels for each row in YYYY-MM-DD format, we can use these names to select out individual or ranges of rows as ```dataset.loc[rowname, colname]``` or ```dataset.loc[startrowname:endrowname, startcolname:endcolname]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074c417-0038-43d1-9e01-cc82795ff531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN THE DATES AND CATCHMENT ID\n",
    "obsplot = obsdata.loc['YYYY-MM-DD':'YYYY-MM-DD', 'CATCHMENT ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3924820-6633-4da7-b831-b417b0aa0cee",
   "metadata": {},
   "source": [
    "Pandas integrates with the [matplotlib](https://matplotlib.org/) plotting library directly, and you can just call ```.plot()``` on a dataframe to get a simple visual representation of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41fb31-71dd-4672-bbac-12881ab29d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0af5790a-e9b8-4a18-9996-d975eb744d34",
   "metadata": {},
   "source": [
    "If we wish to modify the plot, this is also possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec49924-0c44-4b66-9818-bd608cd27a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10}) # make the text in the plot easier to read\n",
    "ax = plt.gca() # gca = get current axis, allows us to get the most recent axis used... \n",
    "\n",
    "# INSERT CODE HERE # ... and specify it to the plotting command, which is simply dataframename.plot()\n",
    "\n",
    "# INSERT CODE HERE # set the y axis lower and upper bounds\n",
    "\n",
    "# INSERT CODE HERE # set the title\n",
    "\n",
    "# INSERT CODE HERE # set the y axis label\n",
    "\n",
    "# INSERT CODE HERE # add a legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562924b-88e4-41ad-a999-d0c7ed0c0ae8",
   "metadata": {},
   "source": [
    "Hmm, some of the data points look a bit weird, suddenly going very low - what's going on there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86a31b-f589-4693-8fd2-8b4f2d7b5db5",
   "metadata": {},
   "source": [
    "It's likely the '-1' values we could see in the table earlier. We can deal with these by telling pandas that this value represents a missing value, so that it ignores them when plotting them and doing any other analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a292747-e853-4d68-8f43-d2c93f103686",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "obsdata = pd.read_csv('s3://example-data/obsflows.csv', \n",
    "                      parse_dates={'Times': [0,1,2]}, dayfirst=True, \n",
    "                      # INSERT CODE HERE\n",
    "                      storage_options={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\", 'anon': True})\n",
    "obsdata.set_index('Times', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae889e9-f91f-4856-b453-a22f4b80a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2245875-44db-4f2a-b237-b84ba5b49fae",
   "metadata": {},
   "source": [
    "Excellent, now let's try the plotting again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81c0ea-64e1-4307-99de-9f80c118288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeperiod and gauge ID to plot\n",
    "startplot = '2000-01-01'\n",
    "endplot   = '2015-12-31'\n",
    "catchmentplot = '39010'\n",
    "\n",
    "obsplot = obsdata[startplot:endplot]\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "ax = plt.gca()\n",
    "obsplot[catchmentplot].plot(ax=ax, label = 'obsflows')\n",
    "ylims = ax.get_ylim()\n",
    "ax.set_ylim([0, ylims[1]])\n",
    "plt.title(catchmentplot)\n",
    "plt.ylabel('Daily mean flow (' + r'$m^3$' + ')')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6514d-7a5d-4ff8-9807-1de3dd938e80",
   "metadata": {},
   "source": [
    "Much better! What pandas has done is simply leave a blank space where the data is NaN, which would show up if you zoomed in the plot a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e3f9c-c6e4-4c39-ab2c-a268c0053221",
   "metadata": {},
   "source": [
    "E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58be9e3-d4fb-4ec8-8112-ef792b7f36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeperiod and gauge ID to plot\n",
    "startplot = '2010-01-01'\n",
    "endplot   = '2010-05-31'\n",
    "catchmentplot = '39010'\n",
    "\n",
    "obsplot = obsdata[startplot:endplot]\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "ax = plt.gca()\n",
    "obsplot[catchmentplot].plot(ax=ax, label = 'obsflows')\n",
    "ylims = ax.get_ylim()\n",
    "ax.set_ylim([0, ylims[1]])\n",
    "plt.title(catchmentplot)\n",
    "plt.ylabel('Daily mean flow (' + r'$m^3$' + ')')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af8d6d-953b-4b77-b750-fd3165c3f6af",
   "metadata": {},
   "source": [
    "Let's have a look at annual averages for a given catchments to see if we can pick out any particularly wet/dry years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17f8eb-8e92-43e9-a18e-50670a22cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e606d-930e-4371-ac83-ba2e7b4f19b2",
   "metadata": {},
   "source": [
    "To check if Pandas has recognised the information in the Times column as date-time information we can check its type. We can access the information in this column directly by using the 'index' accessor, as we set this column as the index earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e670d2-4aa5-4740-9308-15a94cff3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(obsdata.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03f79f-0b3b-4583-8c8a-8d9fa0cc28ea",
   "metadata": {},
   "source": [
    "'DatetimeIndex' is showing up, which means that Pandas has indeed recognised it as a special category of data - date and time information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21602a0-1e65-4db6-ac45-e9d9ada3fd82",
   "metadata": {},
   "source": [
    "We can therefore pull out some useful information from it, such as the year, month, day, but also less obvious information such as day-of-week, quarter/season, days-in-month..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7ee16-a73e-497b-8f92-c7d41aba1aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cab2a0-84f4-410e-b485-7517711a7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cb1e4-66cf-40f5-9370-62ab37b8bdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24b20a7b-4b8c-4ff8-a926-f9e334efcdd1",
   "metadata": {},
   "source": [
    "We can use this information to group the rows of the table into categories, and then perform some operation on these groups. For example, to find the yearly mean for each catchment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5f0c3-2c1d-4ee6-a83f-1fd152eca429",
   "metadata": {},
   "outputs": [],
   "source": [
    "annavgs = obsdata.groupby(# INSERT CODE HERE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f14367-013c-4c1b-b21f-87d13398a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "annavgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bacd6-10a6-4a3a-9357-0e8945abdcb2",
   "metadata": {},
   "source": [
    "Note that the default behaviour of arithmetic operations (such as mean, sum etc.) applied to pandas dataframes where there is missing data is to ignore the missing data when doing the calculation. So for example if a given catchment has a handful of observations that are showing as NaNs for a given year, the mean for that catchment-year will be calculated ignoring the NaN values completely. Only catchment-years where *all* the observations are NaN does a NaN get returned in an arithmetic calculation. This behaviour is customisable. Take a look at the [Pandas documentation](https://pandas.pydata.org/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69aa4d8-578e-4580-bae4-0602468aa44c",
   "metadata": {},
   "source": [
    "Pandas [groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) logic is very powerful and useful, especially this ability to parse dates and times. This example is just one of many things that can be done with it. Another example could be to add a column describing a particular catchment property, like steepness or size, group the catchments into bins from smallest to largest or shallowest to steepest, and see how the mean flow varies with these properties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e57a99-1b4c-43f1-95d5-14c4efafd34f",
   "metadata": {},
   "source": [
    "For now though, let's keep it simple and take a look at a couple of the annual averages we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02683e1e-a551-4aa3-8417-263809c99e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annavgs['39001'].plot(label='thames')\n",
    "axes = plt.gca()\n",
    "annavgs['23001'].plot(ax=axes, label='tyne')\n",
    "plt.legend()\n",
    "\n",
    "corr = np.corrcoef(annavgs['39001'], annavgs['23001'])[0,1]\n",
    "plt.title('R = ' + str(corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3e7cf-c3f0-4d9c-9079-bd7ed8739c48",
   "metadata": {},
   "source": [
    "The Tyne is a large river in the northeast of England. There is perhaps not as much correlation between the two rivers as you might think! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25261a59-b115-4b76-9644-2e1854b8884b",
   "metadata": {},
   "source": [
    "Pandas can also group by meteorological season, which are defined based on purely the calendar months:\n",
    "- Winter: December, January, February (DJF)\n",
    "- Spring: March, April, May (MAM)\n",
    "- Summer: June, July, August (JJA)\n",
    "- Autumn: September, October, November (SON)\n",
    "\n",
    "We can create seasonal averages by making further use of pandas's date-time handling. Perhaps let's look at the 90th percentile of the winter flows in each year for the same two catchments above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda8592-294a-442e-9183-ad2e5088c189",
   "metadata": {},
   "source": [
    "Note that because 'quarters starting in December' is not a property of the Index (unlike years), we have to use a more sophisticated method to group the rows the way we would like.\n",
    "\n",
    "Pandas has a [```Grouper```](https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html) function that allows for more complicated groupings. To see what more it allows, you can run ```pd.Grouper?```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a99a5-1472-47fa-9a3b-acc36b6d9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc90 = obsdata.groupby(pd.Grouper(freq='QS-DEC')).quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fef3b-b8f1-48ce-845c-249ac9253a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92482d76-9133-4a97-8b39-6abed5bc8faa",
   "metadata": {},
   "source": [
    "To select out only the winter flows, we want every 4th row, which we can represent as ```::4```. This is python indexing shorthand for ```0:-1:4``` which translates as the 0th (first) row to the -1th (last) row with a 'stride' of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495a897-829d-4e9a-a95c-cc6d16cef78a",
   "metadata": {},
   "source": [
    "You'll see [```loc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) and [```iloc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) a lot with Pandas. These are powerful methods used to select out rows and columns from the table.[```loc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) can be used to select out cells or ranges of cells using the row and column *names* or *labels*, whereas [```iloc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) is used to select based on the row and column *indices* (e.g. the cell at row 8 and column 5 would be selected with ```obsdata.iloc[8, 5]``` and ```obsdata.loc['1961-01-08', '04003']``` - try it out for yourself!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d6026-0c0c-4837-8b4c-a97a53f585f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc90_DJF = pc90.iloc[::4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef06958-f72d-486a-88c0-74ae7d2d5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc90_DJF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228ad33-7d37-43b3-b7ad-40df87114947",
   "metadata": {},
   "outputs": [],
   "source": [
    "thames = pc90_DJF['39001'].plot(label='thames')\n",
    "axes = plt.gca()\n",
    "tyne = pc90_DJF['23001'].plot(ax=axes, label='tyne')\n",
    "plt.legend()\n",
    "\n",
    "corr = np.corrcoef(pc90_DJF['39001'], pc90_DJF['23001'])[0,1]\n",
    "plt.title('R = ' + str(corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae265f-4cb5-4e74-b082-8a90050562bf",
   "metadata": {},
   "source": [
    "## [ASCII Grid files (ESRI format)](https://en.wikipedia.org/wiki/Esri_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca73fda-9143-43a9-a639-1cf91916c7f2",
   "metadata": {},
   "source": [
    "Another common file you might come across in hydrology is a file with the extension '**.grd**'. These files are often used as inputs/outputs of hydrological models. They are another textual representation of gridded data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38120d14-b446-4917-8421-d127b10a7be6",
   "metadata": {},
   "source": [
    "Ultimately these are just numbers laid out in a (usually) tab-separated pattern with some header lines at the top describing metadata needed to interpret the data into a grid. I find the easiest way of working with these is to read them into [Xarray](https://docs.xarray.dev/en/stable/), the now well established go-to library for working with gridded and N-dimensional data in various formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa2094-2e43-4257-9b05-dbf71b54753a",
   "metadata": {},
   "source": [
    "Here we'll develop the code to do this, but first let's take a look at what a typical grd file looks like if you open it in a text editor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cff9f-dbfe-4560-8f2b-a0d826de08e0",
   "metadata": {},
   "source": [
    "**Bonus:** The next code cell also shows how to display an image stored in an object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078a230-adf4-4ed2-874c-d44ecfea4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "fs_img = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "display(Image.open(fs_img.open('s3://example-data/grdfile.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c63f1e-7bba-475d-be1e-6195707516a2",
   "metadata": {},
   "source": [
    "Here you can clearly see the 6 header lines, followed by the rows & columns of numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46fbe5-af79-462e-a752-6bdac6153211",
   "metadata": {},
   "source": [
    "- The **ncols** and **nrows** information is fairly self-explanatory\n",
    "- The **xllcorner** and **yllcorner** are telling you what the x and y coordinates are of the 'lower left' corner of the grid. Important to note that these numbers represent the *corner* of the gridcell in the lower left corner of the grid, not it's centre point.\n",
    "- **cellsize** tells you the x and y extent of the each cell in the grid, i.e. the distance between gridcells\n",
    "- **NODATA_value** is also fairly self-explanatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a4ac-c39a-4a2b-b25b-0fd0d48f4db0",
   "metadata": {},
   "source": [
    "These files are only able to represent regular grids (grids that have consistent spacing between the grid cells) that also have the same spacing in the x and y direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d646c5-f873-49f5-a0be-fc852b8e896a",
   "metadata": {},
   "source": [
    "The header information contains everything we need to know in order to read this into Xarray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610042e-5501-4051-b869-1b0298517dbf",
   "metadata": {},
   "source": [
    "First we need to read in the original file. Reading in any textual data in python is simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576be3e-f9a8-4f1f-a9d3-b10017e7b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "grdfile = s3.open('s3://example-data/absw_230_2009_06.grd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc9221-88ad-4ad0-b542-9a97e0ef6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdfile.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317d23e-f880-42c3-89dc-aa3c321b3eb8",
   "metadata": {},
   "source": [
    "The [```readline()```](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects) function reads the current line as a string, then the next time it is called, does the same for the next line. To grab the entire file contents at once, we can use ```readlines()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3aa2a-981f-47e5-adac-85a34f216072",
   "metadata": {},
   "outputs": [],
   "source": [
    "filecontents = grdfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022873a-dc5e-4573-a9e2-b3f4d9aa9cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filecontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdaa08c-435e-4d0d-9211-4f15c5f29c7d",
   "metadata": {},
   "source": [
    "Each row becomes an item in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f2a95-0d33-4078-a3db-22685e90933c",
   "metadata": {},
   "source": [
    "**Note:** That we seem to missing the top row of the file. This is because we already called ```readline``` which advances the line number the next call to ```readline``` or ```readlines``` will read from. To read from the top of the file, the file needs to be reopened. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d412c1e-a6a3-4589-ac0b-35b50b1df41b",
   "metadata": {},
   "source": [
    "When we have finished working with a file using this method it is important to manually close it, otherwise it will remain open in memory and can cause all sorts of fun errors and bugs, especially when loops are involved! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9fa32-ce56-480d-a98b-92f5340e149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7ab0b-efcb-440e-a353-85537879995e",
   "metadata": {},
   "source": [
    "We could read in the entire file this way, however for numeric data other libraries, mainly [Numpy](https://numpy.org/), tend to be easier to work with. But now that we know how to read in text, we can pull out the header information which we can then use to construct the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bc7e6-533f-4431-9ede-80110d0e0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = 's3://example-data/absw_230_2009_06.grd'\n",
    "grdfile = s3.open(filein)\n",
    "ncols  = # INSERT CODE HERE\n",
    "nrows  = # INSERT CODE HERE\n",
    "xllc   = # INSERT CODE HERE\n",
    "yllc   = # INSERT CODE HERE\n",
    "res    = # INSERT CODE HERE\n",
    "nodata = # INSERT CODE HERE\n",
    "\n",
    "print('ncols: ' + str(ncols))\n",
    "print('nrows: ' + str(nrows))\n",
    "print('xllc: ' + str(xllc))\n",
    "print('yllc: ' + str(yllc))\n",
    "print('res: ' + str(res))\n",
    "print('nodata: ' + str(nodata))\n",
    "\n",
    "grdfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd25d25-2090-441f-b91e-ab9ed2e9df35",
   "metadata": {},
   "source": [
    "Next we can use [Numpy's ```loadtxt```](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html) function to read in the actual numeric data, skipping the header files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae7d94-e6ef-460e-b38b-ebc6b5824f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdfile = s3.open(filein)\n",
    "data = np.loadtxt(grdfile, skiprows=6)\n",
    "grdfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b1683-7bc8-4aa1-95e6-0150f650029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f0fab-ea5a-4550-b001-1a3ae10c8608",
   "metadata": {},
   "source": [
    "Numpy has cleverly determined the data it is reading in is a grid, and stored it as such in an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdecae37-5548-4907-b875-a1cd3dbafbc8",
   "metadata": {},
   "source": [
    "Now all we need to do is generate the coordinates from the information we read in from the header:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b64416-c30c-44b5-be3b-d13a681d8580",
   "metadata": {},
   "source": [
    "The [Numpy ```linspace```](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) command generates a sequence of equally spaced numbers: ```np.linspace(start, end, number of elements in sequence)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ba763-4313-4256-a75c-c3da6e59b14c",
   "metadata": {},
   "source": [
    "We can work out all of these from the header information. Remember that the xllc and yllc are the coordinates of the *corner* of the lower left gridcell, and generally gridcell coordinates are for the *centre* of each gridcell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7ca03-1ea8-4561-8a70-a31f74811118",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoords = np.linspace(xllc + (res/2), xllc + (res/2) + (res*(ncols-1)), ncols)\n",
    "ycoords = np.linspace(yllc + (res/2), yllc + (res/2) + (res*(nrows-1)), nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a323d4e-0bad-4fd6-a0a5-680bb6248729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xcoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3972c32-0918-450a-ba27-e68700ec1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycoords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace394e-3119-4cbe-8a6a-c0ed09ce7799",
   "metadata": {},
   "source": [
    "Now we are ready for Xarray to take charge. Xarray requires 3 key pieces of information:\n",
    "- The data values themselves\n",
    "- The names of the dimensions/coordinates (in this case they can just be 'x' and 'y')\n",
    "- The coordinates of each of the dimensions (in this case the x and y coordinates we've just generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c6722-65c0-40e7-bd65-7801c49e678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydimname = 'y'\n",
    "xdimname = 'x'\n",
    "grdxr = xr.DataArray(data, coords=[(ydimname, ycoords), (xdimname, xcoords)])\n",
    "grdxr = grdxr.where(grdxr != nodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03fb7f-5684-4684-861d-94eadbbc0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdxr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba09b2-eccc-4422-bd42-3c826b9e49b8",
   "metadata": {},
   "source": [
    "An Xarray DataArray is a coordinate-aware representation of our data, much like a version of Pandas designed for gridded and N-dimensional data, allowing operations and analyses to be carried out based on the *coordinate values*. We will learn more about Xarray soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8928c47-4f4d-469c-af8d-c29bf51f51ea",
   "metadata": {},
   "source": [
    "For now, let's plot the data and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c825384-2004-4e35-bc64-de142b6d03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdxr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237afc6-3aaf-4512-add5-006d7ad6b0ea",
   "metadata": {},
   "source": [
    "Oh dear, that appears to be upside down! How might we fix this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38425d3b-4cc3-4bd2-b41a-389aebbca117",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoords = np.linspace(xllc + (res/2), xllc + (res/2) + (res*(ncols-1)), ncols)\n",
    "ycoords = # INSERT CODE HERE\n",
    "\n",
    "ydimname = 'y'\n",
    "xdimname = 'x'\n",
    "grdxr = xr.DataArray(data, coords=[(ydimname, ycoords), (xdimname, xcoords)])\n",
    "grdxr = grdxr.where(grdxr != nodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415edf81-7372-4692-944e-4d86125f2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdxr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0c7dc-91db-4333-aabe-1b5a148bc367",
   "metadata": {},
   "source": [
    "Now we have a basic Xarray representation of the data, which comes with various powerful analysis tools, some of which we will explore in the next notebook, with data that is a little more interesting than all zeroes! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592239dd-e372-43f2-b7c7-950c783d5914",
   "metadata": {},
   "source": [
    "## Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563eb92-a10b-4f5a-9183-0db8f165e86a",
   "metadata": {},
   "source": [
    "The interested reader might want to check out [Polars](https://pola.rs/), which is a more efficient rewrite of pandas for improved speed in handling large tabular datasets and databases. The commands are largely the same as Pandas. \n",
    "\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "- [Numpy loadtxt documentation](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html)\n",
    "- [Pandas groupby documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n",
    "- [Pandas Grouper documentation](https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html)\n",
    "- [National River Flow Archive (NRFA) catchment information](https://nrfa.ceh.ac.uk/data/about-data/catchment-information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e281d4-53d1-4364-8340-0c006223f8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zarrv3",
   "language": "python",
   "name": "zarrv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

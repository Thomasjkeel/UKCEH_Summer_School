{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51a8b4c-d6e6-4eaa-87ef-dc2d734bd206",
   "metadata": {},
   "source": [
    "## Part 3: Exploring remotely-stored Zarr datasets using Xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b623e1c-1746-4a99-8a50-8723dbedb7db",
   "metadata": {},
   "source": [
    "## A brief intro to multi- or N-dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57baf0ee-1f0a-4cae-b507-8b6a0df821e8",
   "metadata": {},
   "source": [
    "Multi- or N-dimensional geospatial data refers to datasets that include multiple layers or dimensions of geographical information. This data goes beyond the simple two-dimensional latitude and longitude (or y and x) coordinates we have seen in the earlier notebooks and often includes additional dimensions such as time, elevation (altitude or depth), and other variables like temperature, humidity, rainfall.\n",
    "\n",
    "This notebook was designed for a session as part of the UKCEH Summer School. It does not cover all aspects of multi-dimensional data use by the Python scientific communities. Additional resources can be found throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0624f1-e9e4-4448-8815-5548ecd4f939",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Loading N-dimensional datasets from object storage\n",
    "- Using Xarray with Dask to explore N-dimensional datasets\n",
    "- Plotting N-dimensional datasets with Xarray and matplotlib\n",
    "- Computing a climatology\n",
    "- Using shapefiles to \"cut-out\" areas of N-dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32a9a6-b709-4497-8c48-d8fed2d38ebc",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca0836-63f6-4e48-b111-a06f114231ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY NEEDS TO BE RUN IF USING GOOGLE COLABS\n",
    "%%capture\n",
    "!pip install s3fs cartopy zarr boto3 netcdf4 rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784665fe-bd83-47f9-ba02-d81867228d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import boto3\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cartopy as cp\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import datetime\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed951190-28ce-47fc-bf65-3a7b9520e749",
   "metadata": {},
   "source": [
    "In this notebook we will explore how to work with gridded N-dimensional datasets stored remotely. Specifically we will be looking at datasets stored as [Zarr](https://zarr.dev/), which is a format becoming increasingly common in big-data science where datasets need to be stored in remote \"object stores\" due to their size. \n",
    "\n",
    "[NetCDF](https://www.unidata.ucar.edu/software/netcdf/) is another very common format for gridded N-dimensional datasets. Whilst we will not explicitly work with NetCDF data here, once data is read into [Xarray](https://docs.xarray.dev/en/stable/) it behaves in the same way and the same commands can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb0f86-c9ce-47fd-91a3-a4fb460ee885",
   "metadata": {},
   "source": [
    "### Access and explore the datasets in object store buckets (using FSSpec and Xarray)\n",
    "\n",
    "We will be working with an hourly gridded rainfall dataset - [UKCEH GEAR](https://catalogue.ceh.ac.uk/documents/fc9423d6-3d54-467f-bb2b-fc7357a3941f)\n",
    "\n",
    "For publically available data such as this, we do not need any credentials, and instead pass ```anon=True``` which means 'access the data as an anonymous user'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38a7c3-43fc-49e9-94ba-a6d12c65edd3",
   "metadata": {},
   "source": [
    "We will use the [FSSpec](https://filesystem-spec.readthedocs.io/en/latest/) package, which basically has the ability to take any data stored on any storage system and make it look as if the data is on an ordinary disk to the rest of your code. The idea is that you run this bit of code once and then it gets out the way and lets you continue your coding as if the data was still on disk and not in a remote object store in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d849dae-3e4c-49ae-a565-bb90e19e3663",
   "metadata": {},
   "source": [
    "To make it work we provide ```anon=True``` and the ```endpoint_url```, but also the path to the specific dataset we want to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008dd5d3-104a-4f8b-801a-dcf9ad8a0b59",
   "metadata": {},
   "source": [
    "**Note:** There are two ways of doing this, depending on which version of the Zarr package is installed in your python environment. To check, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb353f-a141-4822-8711-335d56bb9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517165e-8b57-4b29-b1a4-bf5010c76e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarr v2 method\n",
    "#zstore = fsspec.get_mapper('s3://gearhrly/gearhrly_fulloutput_yearly_100km_chunks.zarr', \n",
    "#                           anon=True, \n",
    "#                           endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "\n",
    "# zarr v3 method\n",
    "fs = s3fs.S3FileSystem(anon=True, asynchronous=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "zstore = zarr.storage.FsspecStore(fs, path=\"gearhrly/gearhrly_15day_100km_chunks.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d137178-7bac-4b8c-b477-a18190dffe4f",
   "metadata": {},
   "source": [
    "We then pass the created file-system-like object to Xarray, and from here on in Xarray behaves as if the data were stored locally on disk, and we can largely forget about the fact that it's actually stored remotely in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573d9fe-f2f7-448a-8d3f-3b43735f0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(zstore, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e63d1-86ba-4541-8df0-593dcf2f33ae",
   "metadata": {},
   "source": [
    "Next let's explore the dataset a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdd574-041d-47d3-a3c2-743649aec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b51f2c-a0bf-4433-91b3-1545f601e688",
   "metadata": {},
   "source": [
    "The variables in the dataset are split into those that describe the coordinates, and those of the main data. We can see the three main data variables: 'min_dist', 'rainfall_amount' and 'stat_disag', you can click the page icon at the end of each variable's row to find out a little more about each. \n",
    "\n",
    "The names in brackets next to the variable names (the second column of information) show you the dimensions that each variable is on. Here, all our data variables are on a 3D grid of time, y and x. \n",
    "\n",
    "You can see in the Coordinates section that we have other coordinates besides those for the time, y and x dimensions. The lat and lon variables tell you the latitude and longitude conversion for each x,y gridpoint and the 'xxx_bnds' variables tell you the extent/valid range of each datapoint. So for example a gridpoint of (6000,6000) on a grid with a resolution of (1000,1000) would have extents/boundaries of (5500,6500) for both x and y, describing the extent of the gridbox this gridpoint represents. The same concept can be extended to the time-dimension too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d89e31-a94c-42f6-b5e3-9bb3688d5a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "fs_img = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "display(Image.open(fs_img.open('s3://example-data/Scratch-1.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cd751-64f5-437d-abf8-f20df297e5ad",
   "metadata": {},
   "source": [
    "More information about the dataset is available by clicking on 'Attributes' to expand this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed17747-892a-467d-939d-bdad2e2f072e",
   "metadata": {},
   "source": [
    "All Zarr datasets that follow the [CF-Conventions](https://cfconventions.org/) (a format standard for NetCDF, and Zarr by extension, originally created for climate-model data but slowly being expanded out to cover more disciplines in the geosciences) should look similar to this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28738fa-f57c-485d-9fd3-af0f6d36d8f1",
   "metadata": {},
   "source": [
    "Variables are selected using ```datasetname['variablename']``` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464f2af-90f0-43ba-886b-344a16b08285",
   "metadata": {},
   "source": [
    "The ```.data``` attribute pulls out the underlying data array of the specified variable, which in the case of Zarr will be a Dask array. Dask arrays, as opposed to standard Numpy arrays you might be used to working with, are 'chunked' into little parcels of data. It's a feature of the format that makes it most suitable for storage on the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d535226-0ee4-4acb-afc5-028658607b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['rainfall_amount'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34d35b-2199-41f9-a2c4-bc51faa206d4",
   "metadata": {},
   "source": [
    "- **Bytes** shows you the overall size of the array and indivdual chunk\n",
    "- **Shape** shows you the dimension sizes of the array and chunk. This dataset has 236688 time steps!\n",
    "- **Dask graph** tells you how many tasks (68432 here) would be needed to load the actual data into memory (see below section on Lazy Loading)\n",
    "- **Data type** tells you what the data inside the array consists of. Here it is a Numpy array containing 64-bit (8-byte) floats (floating-point numbers), which is typical for numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f752d-085b-4328-bd76-a2600cdb9089",
   "metadata": {},
   "source": [
    "### Computations and visualisations with Xarray and Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c1d20-ad4e-4064-b9d5-5b2165b0870e",
   "metadata": {},
   "source": [
    "#### An introduction to Lazy Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829160-7078-4751-bc10-1164f71bc40e",
   "metadata": {},
   "source": [
    "Before we go any further it is very important to know that Xarray loads data *lazily*. This means that the actual data is never loaded or computed until it absolutely has to be. So you may run some code that computes the mean of the dataset (e.g. ```dmean = ds['variable'].mean(dim='time')```), but the computation will only actually be carried out when you want to *see* the result of this calculation, e.g. through a plot or print statement, or saving to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1129e-91ba-4864-b67a-2b91761b3c1d",
   "metadata": {},
   "source": [
    "[Dask](https://www.dask.org/) also takes this a step further. Dask is a library designed to automatically parallelise computations; parallelisation is essential when dealing with chunked data such as Zarr on the cloud. Dask sits in the background working out how best to parallelise your computations, then whenever Xarray actually triggers a compute, Dask will automatically kick in and process the computation in parallel. However, with Dask, the *output* of the computation will not persist in memory, even if you save the output to a variable (e.g. ```dmean = ds['variable'].mean(dim='time')```). If you run the code again, Dask will *recompute* everything from scratch, the actual data is not stored in the variable ```dmean```, rather the Dask-instructions for computing it. This can make code very slow if not handled correctly. The best way to handle this is to make use of Dask's [```.persist()```](https://distributed.dask.org/en/latest/api.html?highlight=persist#distributed.Client.persist)and [```.compute()```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) methods. Appending this on to the end of a calculation, such as ```dmean = ds['variable'].mean(dim='time').compute()``` will keep the data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8803cb5-3815-4c77-8fb1-654064dd6973",
   "metadata": {},
   "source": [
    "The main difference between [```persist```](https://distributed.dask.org/en/latest/api.html?highlight=persist#distributed.Client.persist) and [```compute```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) is that [```persist```](https://distributed.dask.org/en/latest/api.html?highlight=persist#distributed.Client.persist) will allow you to continue coding whilst dask computes the result in the background, whereas [```compute```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) will wait until the computation is complete before letting you continue coding. I tend to find [```compute```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) behaves the most intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d0e4-2f54-4a3d-9f3b-fd0b8310b2b1",
   "metadata": {},
   "source": [
    "Now with that out the way let's get to actually looking at the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55523b29-c5eb-4916-8f6a-277af5f83279",
   "metadata": {},
   "source": [
    "#### Plot an individual time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cfca3-90f6-4915-b3ba-b03b7e081d6c",
   "metadata": {},
   "source": [
    "You can index the array and pull out a single timestep like you can any standard array-like object in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1d9ff-87e1-4b1a-b0ca-ddddedaa28bc",
   "metadata": {},
   "source": [
    "Xarray also adds a ```plot()``` method you can call to produce a rough-and-ready quick plot of the data you've selected, in this case the 37th timestep of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fe1af-60c8-4913-b463-9b9edf7b4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ds['rainfall_amount'][36,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a3ec9-5cc0-458a-98c0-715c011f3ac9",
   "metadata": {},
   "source": [
    "#### Customising plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb948c-b6e1-4e9a-b03c-da54341fa090",
   "metadata": {},
   "source": [
    "These plots can be customised to look nicer too. Note that we're making use of Dask's [```compute()```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) method to save the variable ```plotpoint``` in memory, so that we don't have to rerun the processing needed to extract this point from the cloud whenever we rerun the plotting commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296bdcd8-29ad-4023-9558-2168e26aa404",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    plotpoint = ds['rainfall_amount'][36,:,:].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc314d8d-4321-4bb1-b7f7-d1a4d4c70318",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpoint.plot.pcolormesh('lon', 'lat', cmap='Blues', robust=True)\n",
    "plt.title(r'Rainfall $2^{nd}$ Jan 1990 12:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb4064-6c6f-4327-b531-8e0929fd5a73",
   "metadata": {},
   "source": [
    "Here we have:\n",
    "- changed the axes to use lon/lat instead of x/y coordinates by specifying the dataset name of the longitude and latitude variables in the plotting command\n",
    "- changed the colourmap to a nicer one (all pre-built colourmaps can be found at https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html)\n",
    "- changed the title to better describe the data being plotted\n",
    "- used Xarray's 'robust' option, which modifies the colourbar to not stretch to the maximum of the data if it is an outlier. This stops a small area of very high values dominating the colourbar and making other variation invisible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed402b5-e72a-4f3f-860d-3f64ae03fc1a",
   "metadata": {},
   "source": [
    "But it would also be nice to have some coastlines. For this we have to go beyond what Xarray's built-in plotting can acheive and use the Cartopy package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f65c4-4028-4076-8ee3-63ab0994af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs # the set of map projections cartopy supports\n",
    "import cartopy as cp # the full cartopy package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1411d07-ef42-48de-836e-cc2004976e5d",
   "metadata": {},
   "source": [
    "Passing the ```projection``` 'key-word argument' to the plotting function tells the plotting library to invoke cartopy to draw the plot using a given map projection. Here we are using the 'OSGB' map projection (a cartesian grid, in other words a flat plane approximation that ignores the curvature of the earth) which a lot of UK hydrological data will be on. For any data that is on a 'lon/lat' grid instead of an 'x/y' grid, the [```ccrs.PlateCarree()```](https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html#platecarree) projection is a better option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb005d2-6f6f-4d80-8933-684242d2be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = plotpoint.plot.pcolormesh(cmap='Blues', robust=True, subplot_kws=dict(projection=ccrs.OSGB())) # create the initial plot\n",
    "plt.title(r'Rainfall $2^{nd}$ Jan 1990 12:00')\n",
    "plot1.axes.coastlines() # add coastlines\n",
    "gl = plot1.axes.gridlines(draw_labels=True, alpha=0.5) # add gridlines. The alpha parameter is the transparency between 0 and 1. \n",
    "gl.top_labels = False\n",
    "gl.right_labels = False # remove the top and right gridlines labels to make the plot look nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2735b3-a62b-42e1-9184-b08589fd0c57",
   "metadata": {},
   "source": [
    "#### Animations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630157c-ca56-4099-82a8-ea7cc0e6eb82",
   "metadata": {},
   "source": [
    "It might also be nice to see how the data evolves over time in a video or GIF. This is a little more complicated to achieve, so I've broken it down into steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af69f5-fbfd-4065-8f84-bbe86c037491",
   "metadata": {},
   "source": [
    "First extract out the data that you want in the animation, making sure to call ```.compute()``` so that the actual data is stored in memory, otherwise every drawing of a frame of the animation will re-download the data from the cloud which takes time and unnecessary bandwith. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88600c3a-6f69-46a0-837a-f9cf5d8b3fe3",
   "metadata": {},
   "source": [
    "**Note:** However we *don't* have to do this with the time coordinates as they *are* actually loaded in to memory when we first read in the data. You can see this if you take a look at the dataset (by just running ```ds```) - all the variables are dask arrays *except* for ```t```, ```x``` and ```y``` (the coordinate variables) for which you can see the start and end of their actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d242b-ab19-4e7e-b198-3ac0226c8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    data = ds['rainfall_amount'].sel(time=slice('2014-02-01', '2014-02-07')).compute()\n",
    "timecoords = data.time.dt.strftime('%Y-%m-%d %H:%M').values # extract out string representations of the time coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5f5c1-6ff9-41a7-821d-26bc7ce18746",
   "metadata": {},
   "source": [
    "Set some key variables used in the plotting and animating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27253322-e115-4063-a1cd-acad12f5d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xname = 'x' # name of the x dimension\n",
    "yname = 'y' # name of the y dimension\n",
    "timname = 't' # name of the t dimension\n",
    "starttime = timecoords[0] # start datetime of the animation (e.g the first time in the data)\n",
    "outname='./example_animation' # where to put the file + output filename without the file extension\n",
    "title='Hourly Rainfall' # title to put on plot\n",
    "colmap='Blues' # colour map to use, see https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "vmin=0 # lower colourmap boundary of all plots (the colourbar needs to be the same across all plots)\n",
    "vmax=data.quantile(0.98).compute() # upper colourmap boundary of all plots, using the 98th percentile of the data here\n",
    "ext='mp4' # gif or mp4 (mp4 is recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0bb584-14af-4628-9fd8-11361423b36f",
   "metadata": {},
   "source": [
    "Before animating, we set up the first frame to check it all looks ok. Note that the animation is done by the matplotlib plotting library directly, so we plot the data using matplotlib directly instead of Xarray's built-in plot function. The commands are ultimately very similar, as the Xarray version is just a wrapper around matplotlib's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ae3e8-8550-4850-b47f-ee104a210434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory to store the animation in if it doesn't exist\n",
    "dirname = os.path.split(outname)[0]\n",
    "if not os.path.exists(dirname):\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "# extract out x and y coordinates\n",
    "xs = data[xname].values\n",
    "ys = data[yname].values\n",
    "\n",
    "############################# CREATE THE FIRST FRAME ###########################\n",
    "#\n",
    "# set up the plot with coastlines\n",
    "fig = plt.figure()\n",
    "ax1 = plt.axes(projection = ccrs.OSGB())\n",
    "ax1.coastlines()\n",
    "\n",
    "# add in the x/y axis labels\n",
    "gl = ax1.gridlines(draw_labels=True, alpha=0.5)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# set the title\n",
    "ax1.set_title(title + ' ' + starttime)\n",
    "\n",
    "# plot the first frame using matplotlib directly\n",
    "pcm = ax1.pcolormesh(xs, ys, data.sel(time=starttime).values, vmin=vmin, vmax=vmax, cmap=colmap)\n",
    "# manually create a colourbar to the left of the main plot\n",
    "cbaxes=fig.add_axes([0.15, 0.1, 0.05, 0.8])\n",
    "plt.colorbar(pcm, cax=cbaxes, extend='max') # extend the upper limit of the colourbar to make clear any values higher than it are the same colour\n",
    "cbaxes.yaxis.set_label_position(\"left\") # set the colourbar label position...\n",
    "cbaxes.yaxis.tick_left() # ...and tickmarks\n",
    "# set the ylabel of the colourbar to the xarray dataarray name and failing that, leave it blank\n",
    "try:\n",
    "    plotlab = data.name + '  ' + data.units\n",
    "except AttributeError:\n",
    "    plotlab = ''\n",
    "cbaxes.set_ylabel(plotlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2dafb-fe4f-4cad-9c5b-852bacaafbb8",
   "metadata": {},
   "source": [
    "Now we can write the animation function. This defines what we want to happen for each frame of the video. The input to the function can only be the frame number (```f``` here). The output of the function needs to be the output of the plotting command. \n",
    "\n",
    "The first thing we have to do is clear the existing plot (the previous frame), then plot the next timestep like we would if we were just creating a static plot, using the frame number to select the next timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b38017-a56a-42a3-ac05-39d3cdb92c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################## CREATE THE ANIMATION FUNCTION #################################\n",
    "#\n",
    "# The animation function\n",
    "# This defines what we want to happen everytime matplotlib plots a new frame\n",
    "# It has to have one input, the frame number, and one output, the 'plot handle'\n",
    "# (the object returned when you plot something)\n",
    "def animate(f):\n",
    "    print('Plotting frame ' + str(f+1)) # tell us what frame we're on\n",
    "    ax1.clear() # clear the axes of the current plot\n",
    "    ax1.coastlines(resolution='10m') #redraw the coastlines\n",
    "    # redraw the axis labels\n",
    "    gl = ax1.gridlines(draw_labels=True, alpha=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    # re-set the title, noting that we are incrementing the date by one day each time\n",
    "    ax1.set_title(title + ' ' + timecoords[f])\n",
    "    # replot the data, noting that we are incrementing through the time axis,\n",
    "    # plotting the next frame each time\n",
    "    pcm = ax1.pcolormesh(xs, ys, data.isel(time=f).values, vmin=vmin, vmax=vmax, cmap=colmap)\n",
    "    return pcm,\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429943b7-357a-4697-95c4-2188e6a8401e",
   "metadata": {},
   "source": [
    "Then we instruct matplotlib to create the animation and save it as a gif or mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd0989-efc0-4833-b71e-b27af701fce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the animation function to create the animation\n",
    "# you need to provide the figure handle, animation function, \n",
    "# and frame numbers you want to plot. Setting 'blit=True' means that\n",
    "# only the bits of the plot that actually change from frame to\n",
    "# frame will be redrawn. This is much more efficient.\n",
    "nframes = len(timecoords)\n",
    "print('Total frames = ' + str(nframes))\n",
    "anim = animation.FuncAnimation(fig, animate, range(0,nframes), blit=True)\n",
    "\n",
    "# save the plot using ffmpeg to create an mp4, or convert to create a gif\n",
    "# mp4 is much more efficient in terms of file size and memory\n",
    "# fps = frames per second (speed)\n",
    "# dpi = dots per inch (resolution)\n",
    "# bitrate = video quality (less/more compression)\n",
    "if ext=='mp4':\n",
    "    anim.save(outname + '.mp4', fps=10, dpi=300, bitrate=10000, writer='ffmpeg')\n",
    "elif ext=='gif':\n",
    "    anim.save(outname + '.gif', writer='imagemagick', fps=10, dpi=300)\n",
    "else:\n",
    "    raise Error('format ' + ext + 'not supported')\n",
    "\n",
    "print('Animation saved to ' + outname + '.' + ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b4457-f492-4d79-a427-5ff3bbd0233d",
   "metadata": {},
   "source": [
    "And view it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1167d9-3213-46f6-9953-9ff55bf15993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "fs_img.get('example-data/example_animation.mp4', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642383dc-ec68-437d-8bfb-bd2ea3ef5540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"example_animation.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4447be-1b4d-4289-ab68-32d12293885d",
   "metadata": {},
   "source": [
    "This might not work on all browsers/notebook servers. If you can't get it to show, try downloading it from the files section on the left. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3197991-39c7-447d-a464-15411de1903c",
   "metadata": {},
   "source": [
    "#### Climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e8df9-3d2b-4833-9e0a-2aef4bc4bfdb",
   "metadata": {},
   "source": [
    "We can also do things such as computing the monthly climatology. This takes a while to compute using the in-built default dask setup, so this is more \"something for you to try at home\". It is best to run it once and save it to disk so that it can be loaded in when you next want to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d952bd2-b38c-4a35-b5be-aae1d4350282",
   "metadata": {},
   "source": [
    "To that end, you only need to run the next cell if you want to generate the 'gear1hrly_climatology.nc' file yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32e2cf-c1e9-4a57-be2c-8ea1633d3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    climo = ds['rainfall_amount'].groupby('time.month').mean()\n",
    "    climo = dailyavg.compute()\n",
    "\n",
    "climo.to_netcdf('gear1hrly_climatology.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c229d5-b39e-4b2a-9803-fac5aa0432f3",
   "metadata": {},
   "source": [
    "Otherwise you can just load it in like you would any netcdf dataset, either from disk, if that's where you've saved it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b207fbc-e66d-4e8c-a870-4ef140ab0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo = xr.open_dataarray('gear1hrly_climatology.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a8ea9-5e27-4ae3-9a0b-16409d2aeae7",
   "metadata": {},
   "source": [
    "Or from object storage, where this a copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a1503-0eba-47ad-b5c2-534a19976d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_climo = s3fs.S3FileSystem(endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\", anon=True) # or anon=False to use default credentials\n",
    "climo = xr.open_dataset(fs_climo.open('example-data/gear1hrly_climatology.nc', 'rb'), engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d27f1-dd7d-4865-bb6c-cc6789faa4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "climo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072c6e0-541c-44a4-9760-ca6a7068d510",
   "metadata": {},
   "source": [
    "Once again we can easily produce a quick plot of the data, in this case the 2nd month of the climatology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e84b42-3ad6-4c03-8a17-45ec2e9f0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo['rainfall_amount'][1,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c6864-ab8f-4cb7-9b90-ce26f14bf61d",
   "metadata": {},
   "source": [
    "Or a plot comparing two different months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa9410-a091-4e9f-a055-acad977c8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap='Blues' # colour map to use, see https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "vmin=0 # lower colourmap boundary of all plots (the colourbar needs to be the same across all plots to enable an easy comparison)\n",
    "vmax=0.5 # upper colourmap boundary of all plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f325b-e8c4-4b13-bc44-990d6c7eb9c5",
   "metadata": {},
   "source": [
    "Once again we use matplotlib directly here to create a nicer looking plot. \n",
    "\n",
    "First of all we set up the figure to contain 2 plots, then it's a case of creating the nicer plot as we did earlier. The only difference we need to make is to manually handle the colourbar ourselves, otherwise matplotlib will plot two identical colourbars. Therefore we create a third set of axes to hold the colourbar in a position we define and set the min and max ourselves too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950a544-f9af-4950-a85c-b07109968c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, constrained_layout=True, subplot_kw=dict(projection=ccrs.OSGB()))\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "# extract out x and y coordinates\n",
    "xs = climo['x'].values\n",
    "ys = climo['y'].values\n",
    "\n",
    "plot1 = ax1.pcolormesh(xs, ys, climo['rainfall_amount'][0,:,:].values, vmin=vmin, vmax=vmax, cmap=colmap)\n",
    "plot2 = ax2.pcolormesh(xs, ys, climo['rainfall_amount'][7,:,:].values, vmin=vmin, vmax=vmax, cmap=colmap)\n",
    "\n",
    "# manually create a colourbar to the left of the main plot\n",
    "cbaxes=fig.add_axes([-0.1, 0.1, 0.05, 0.8])\n",
    "plt.colorbar(plot1, cax=cbaxes, extend='max') # extend the upper limit of the colourbar to make clear any values higher than it are the same colour\n",
    "cbaxes.yaxis.set_label_position(\"left\") # set the colourbar label position...\n",
    "cbaxes.yaxis.tick_left() # ...and tickmarks\n",
    "# set the ylabel of the colourbar to the xarray dataarray name and failing that, leave it blank\n",
    "try:\n",
    "    plotlab = climo['rainfall_amount'].name + '  ' + climo['rainfall_amount'].units\n",
    "except AttributeError:\n",
    "    plotlab = ''\n",
    "cbaxes.set_ylabel(plotlab)\n",
    "\n",
    "ax1.set_title('Rainfall Jan average')\n",
    "ax2.set_title('Rainfall Jun average')\n",
    "ax1.axes.coastlines() # add coastlines\n",
    "ax2.axes.coastlines() # add coastlines\n",
    "gl1 = plot1.axes.gridlines(draw_labels=True, alpha=0.5) # add gridlines. The alpha parameter is the transparency between 0 and 1. \n",
    "gl2 = plot2.axes.gridlines(draw_labels=True, alpha=0.5) # add gridlines. The alpha parameter is the transparency between 0 and 1. \n",
    "gl1.top_labels = False\n",
    "gl1.right_labels = False\n",
    "gl2.top_labels = False\n",
    "gl2.right_labels = False # remove the top and right gridlines labels to make the plot look nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a759efa-d06b-4bd5-ac50-508051f26ed3",
   "metadata": {},
   "source": [
    "We can also select out a particular gridpoint to view the climatology there, the syntax is fairly intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a7d6a-9883-4d4b-a75b-3f1318bf871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo['rainfall_amount'].sel(x=275000, y=300000).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b704150-f8ea-4dea-b734-43cefdc6d0f2",
   "metadata": {},
   "source": [
    "We could also compare this to a different gridpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca818c-fe4c-4d8f-b159-1cedca32baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "climopoint1 = climo['rainfall_amount'].sel(x=275000, y=300000)\n",
    "climopoint2 = climo['rainfall_amount'].sel(x=450000, y=200000)\n",
    "\n",
    "point1 = climopoint1.plot()\n",
    "ax1 = plt.gca()\n",
    "point2 = climopoint2.plot(ax=ax1)\n",
    "\n",
    "point1[0].set_label('275000, 300000')\n",
    "point2[0].set_label('450000, 200000')\n",
    "plt.legend()\n",
    "plt.title('Climatology of two grid points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856df7a-da43-43ae-b620-58032e00d41b",
   "metadata": {},
   "source": [
    "### Catchment Extraction \n",
    "\n",
    "Or \"Subsetting gridded data using a shapefile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db75ef4-14cc-4eca-914b-8d82a3e00711",
   "metadata": {},
   "source": [
    "Let's now combine some of the datatypes we've been talking about. Specifically, NetCDF/Zarr and shapefiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd94d05-0f90-4b75-82c5-7e1b097254fe",
   "metadata": {},
   "source": [
    "**Note:** This section uses some functions that I have written(/heavily copied from stackoverflow) and that exist in a python script *utilities.py* in the repo this notebook was obtained from. To use these functions you will also have to have this script in the same folder as this notebook. We can obtain it using ```wget```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4820d88-98d7-4378-804a-373c0eced9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/NERC-CEH/UKCEH_Summer_School/raw/refs/heads/main/Workshop_3/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe845eb4-10cd-4b40-96c1-e079d5de6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39457757-1f2f-4caf-881e-9ae8b709bbc5",
   "metadata": {},
   "source": [
    "First let's load in the climatology we saved earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ccfbf-934e-4f3e-85dc-39fffdf1353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_climo = s3fs.S3FileSystem(endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\", anon=True) # or anon=False to use default credentials\n",
    "\n",
    "climo = xr.open_dataset(fs_climo.open('example-data/gear1hrly_climatology.nc'), engine='h5netcdf')\n",
    "print(climo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c25a0-cead-4ba8-8b8f-42938fb6c20a",
   "metadata": {},
   "source": [
    "And have a look at the shapefile we're going to be using to subset the data using Geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b13320-ac76-4b18-83bd-196355e857a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfname = 's3://example-data/gb_catchments.zip'\n",
    "fs_shp = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "sfile = gpd.read_file(fs_shp.open(sfname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabac91-a292-4441-b7bf-984730564913",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbee70-8f8c-413f-b9a1-56a000a4c2e5",
   "metadata": {},
   "source": [
    "Remember each shape in the shapefile is represented by a row in the table, and the metadata associated with each shape by the columns. The geometry column is the special case, and the main difference between the standard Pandas library and Geopandas. This column contains the vector information that describes the actual shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a0978-51aa-46ea-900a-1e624756ba71",
   "metadata": {},
   "source": [
    "We can use the subsetting script to cut out a piece of our dataset according to one or more shapes in the shapefile:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff83e66-3265-462f-9a5f-c60d6c2681d9",
   "metadata": {},
   "source": [
    "This is the convenience function I've developed to ease the process. Put a question mark after a given function, to see more information about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdd0f6-2fd0-449b-b920-84d9d881b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_subset_shapefile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac76ee2-4fc0-4a9e-8832-ed2efdefdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo39001 = catchment_subset_shapefile(data=climo, sfname=sfname, \n",
    "                                        endpoint=\"https://fdri-o.s3-ext.jc.rl.ac.uk\", \n",
    "                                        IDname='ID_STRING', IDs=['39001'], drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2f63f-b74a-4f19-abf2-e7a9f8be4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo39001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd661f-1815-4846-8a1a-66252382f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo39001_jan = climo39001['rainfall_amount'][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae143af8-9108-47b9-aa40-3261f9b5d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "climo39001_jan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b1be5-e191-41b6-9769-dc950f53aa7e",
   "metadata": {},
   "source": [
    "Now like before let's make the plot a bit better, this time adding in the border of the catchment we've extracted:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccb941-7d08-487e-a090-dd5a0ef49680",
   "metadata": {},
   "source": [
    "Read in the shapefile with Geopandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba138a1-53b9-47f0-b392-ac9663945afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfname = 's3://example-data/gb_catchments.zip'\n",
    "fs_shp = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n",
    "shapefile = gpd.read_file(fs_shp.open(sfname))\n",
    "shapefile = shapefile.set_index('ID_STRING')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b5901-b8af-4264-bc21-18f35a49c5f5",
   "metadata": {},
   "source": [
    "Extract out the same catchment that we used earlier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea05b3-499a-47b0-b1dc-3ba689c38aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat39001 = sfile.where(sfile['ID_STRING']=='39001').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7e388-ccda-41e0-8333-0a5c7d6c1db3",
   "metadata": {},
   "source": [
    "Create the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e943d7-5907-42de-8b57-2094210ba31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbax = plt.axes(projection=cp.crs.OSGB()) # create the (geo) axes for the plot\n",
    "cat39001.plot(ax=gbax, facecolor='None', edgecolor='black', zorder=1) # plot the catchment shapefile, transparently (facecolor='None') and on top of the data (zorder=1)\n",
    "rivers = cp.feature.NaturalEarthFeature('physical', 'rivers_lake_centerlines', '10m', edgecolor='blue', facecolor='none', lw=0.5) # add in the river thames from a public data source (Natural Earth)\n",
    "gbax.add_feature(rivers)\n",
    "climo39001_jan.plot(ax=gbax, cmap='Blues', zorder=0) # add the data we extracted earlier to the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71148ba5-917b-454b-84c4-4703dc85547e",
   "metadata": {},
   "source": [
    "Feel free to take the code in utils.py along with that in these notebooks and use it as you see fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a5657-8869-49f2-ab2d-318937806c36",
   "metadata": {},
   "source": [
    "## Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee72cf3-e627-41be-8a57-b9ee7873bebd",
   "metadata": {},
   "source": [
    "- [Xarray documentation](https://docs.xarray.dev/en/stable/)\n",
    "- [Xarray \"common usage patterns\" tutorial](https://tutorial.xarray.dev/intermediate/01-high-level-computation-patterns.html)\n",
    "- [Dask documentation](https://docs.dask.org/en/stable/index.html)\n",
    "- [Dask tutorial](https://ncar.github.io/dask-tutorial/notebooks/00-dask-overview.html)\n",
    "- [FSSpec documentation](https://filesystem-spec.readthedocs.io/en/latest/)\n",
    "- [Pangeo tutorial gallery](https://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/index.html)\n",
    "- [Object storage tutorial](https://github.com/NERC-CEH/object_store_tutorial)\n",
    "- [**2024** Summer School workshop notebook 1](https://github.com/hydro-jules/school/blob/main/HJ-SS_Workshop-4/HJ-SS_Workshop-4.ipynb), focusing more on NetCDF\n",
    "- [**2024** Summer School workshop notebook 2](https://github.com/hydro-jules/school/blob/main/HJ-SS_Workshop-5/HJ-SS_Workshop-5.ipynb), focusing more on Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd2803-35b3-4076-bbbb-165e8a12962c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zarrv3",
   "language": "python",
   "name": "zarrv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

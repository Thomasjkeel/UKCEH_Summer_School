{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-vtW3o6joJUeGOgYt-zEbJFZU3F-SCoA","timestamp":1751300453723},{"file_id":"1FkK65tZG-_luf1NqmIJpVy8cyHgR-qdn","timestamp":1748419176375}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Workshop 2: Accessing Remote Hydrological Data\n","\n","Accessing data from remote servers is a common task in environmental and climate sciences, where large datasets are often stored on institutional or public repositories. Several tools and protocols are available to facilitate this, depending on the format, structure, and access restrictions of the data. In this notebook we explore some commonly used methods.\n","\n","\n","[2.1. Manual data downloading](#manual_downloading)\n","\n","[2.2.   Command line downloading](#cl_downloading)\n","\n","[2.3.   Data access without downloading](#remote_access)\n","\n","\n"],"metadata":{"id":"5A1jF4VjaVcp"}},{"cell_type":"code","source":["%%capture\n","# Installing packages that are required above the ones already installed in Google Colab\n","!pip install zarr cftime s3fs netCDF4==1.6.0\n","\n","# If you are running this notebook in a platform other than Google Colab, please use the following command to install all the required packages.\n","#!python -m pip install -r requirements.txt"],"metadata":{"id":"mNhfD5abRtWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='manual_downloading'></a>\n","# 2.1. Manual Data Downloading\n","Direct data download refers to the process of retrieving datasets from a remote server via a straightforward HTTP, HTTPS, or FTP/SFTP link. This method is commonly used when data are stored as static files (e.g., NetCDF, CSV, GRIB) and made accessible through a direct URL. Users can download files manually through a browser. While this approach is simple and widely supported, it may be less efficient for accessing large datasets or performing repeated queries on multiple files, in which case more advanced methods may be preferable. Some tools are available, such as [Filezilla](https://filezilla-project.org/), for accessing FTP/SFTP servers for bulk downloads.\n","\n","***Demonstration examples:***\n","\n","https://environment.data.gov.uk/hydrology/explore\n","\n","https://portal.grdc.bafg.de/applications/public.html?publicuser=PublicUser#dataDownload/Home\n"],"metadata":{"id":"PfEa4tNLEbBl"}},{"cell_type":"markdown","source":["<a id='cl_downloading'></a>\n","# 2.2. Command Line Downloading\n","\n","Datasets hosted on remote servers can be downloaded via HTTP, HTTPS, or FTP links using command-line tools in Linux. These methods are especially useful for automating bulk downloads from static URLs. With Linux shell scripting, such downloads can also be parallelised to efficiently handle multiple files simultaneously. This method downloads the full data available via the link being used, and most of the time cannot download subsets of one file.\n","\n","[Parallel computing](https://www.geeksforgeeks.org/computer-science-fundamentals/introduction-to-parallel-computing/) can significantly speed up the process of downloading large numbers of files from the command line. Instead of downloading files one at a time, multiple downloads can be done simultaneously, making efficient use of available CPU and network resources. This approach is especially useful when working with large datasets with multiple files or when accessing data from remote servers with high latency (i.e., delay in communication between your computer and the remote server).\n","\n","***Demonstration examples:***\n","\n","https://catalogue.ceh.ac.uk/documents/dbf13dd5-90cd-457a-a986-f2f9dd97e93c\n","\n","https://www.ncei.noaa.gov/pub/data/\n"],"metadata":{"id":"3-vECW7DaLPf"}},{"cell_type":"markdown","source":["*Note: The ! operator in Python-based Jupyter Notebooks allows users to execute Linux shell commands directly from within the notebook, effectively stepping out of the Python environment to run system-level commands.*"],"metadata":{"id":"Uc-WWEvhm_gE"}},{"cell_type":"markdown","source":["### (i) Downloading single file"],"metadata":{"id":"K_fwJxCqatFU"}},{"cell_type":"code","source":["!wget https://catalogue.ceh.ac.uk/datastore/eidchub/dbf13dd5-90cd-457a-a986-f2f9dd97e93c/GB/monthly/CEH_GEAR_monthly_GB_1894.nc"],"metadata":{"id":"_F8lHLmZ6SL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For now we are deleting the data\n","!rm CEH_GEAR_monthly_GB_1894.nc"],"metadata":{"id":"go6bwCSGDloD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (ii) Downloading restricted data"],"metadata":{"id":"6RApmhhGa1OU"}},{"cell_type":"code","source":["# To use wget for multiple files or even ftp servers which are linked to your account and password protected you can use the following version\n","# For detailed example of accessing servers with password protection please see: https://eidc.ac.uk/help/getdata/downloadData\n","# Add your own username and password to download the whole catalogue.\n","# Please do not try this during the training session as it would take a lot time and storage space.\n","!wget --user=YOUR_USERNAME --password=YOUR_PASSWORD --auth-no-challenge https://catalogue.ceh.ac.uk/datastore/eidchub/dbf13dd5-90cd-457a-a986-f2f9dd97e93c"],"metadata":{"id":"TAXwig0o6pxi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (iii) Downloading multiple files"],"metadata":{"id":"xDZM4EkybIQ8"}},{"cell_type":"code","source":["# Multiple files can be available to us sometimes in compressed format like the following\n","!wget https://www.ncei.noaa.gov/pub/data/hourly_precip-3240/01/3240_01_1948-1998.tar.Z"],"metadata":{"id":"vpo6PT5e97OR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# There are command line methods to uncompress the datasets\n","!tar -zxvf 3240_01_1948-1998.tar.Z"],"metadata":{"id":"5VUkKI36eSzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For now we are deleting the data\n","!rm 3240_*"],"metadata":{"id":"cuhDYIV9Ak0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To download multiple files via wget, you can use a txt file that has a list of the URLs\n","# First as an example we create a txt file with urls we want to download\n","url_list = ['https://www.ncei.noaa.gov/pub/data/daily-grids/beta/by-month/1951/01/prcp-195101-cen-scaled.csv',\n","            'https://www.ncei.noaa.gov/pub/data/daily-grids/beta/by-month/1952/01/prcp-195201-cen-scaled.csv']\n","\n","with open(\"urls.txt\", \"w\") as outfile:\n","    outfile.write(\"\\n\".join(url_list))\n","\n","!more urls.txt"],"metadata":{"id":"cIKKbv_ACDkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Then use the text file with the URL list to download multiple files\n","!wget -i urls.txt"],"metadata":{"id":"_kZO4roRRUQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For now we are deleting the data\n","!rm prcp* urls.txt"],"metadata":{"id":"5QO5lc9HDdl5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='remote_access'></a>\n","# 2.3. Access Without Downloading\n","\n","Hydroclimate data can be accessed remotely without downloading entire datasets. Multiple platforms allow users to query, subset, and stream data directly into analysis environments such as Python, R, or MATLAB. Some methods, such as APIs and FTP/HTTP access, support dynamic data slicing using query parameters. However, remote data access more broadly enables scalable, on-demand computing by allowing users to process and analyze data without the need for local storage. Such remote access methods are increasingly critical for handling the growing volume of high-resolution climate data."],"metadata":{"id":"bwDm7F2IRXG4"}},{"cell_type":"markdown","source":["## 2.3.1. Data Access Protocols/Servers\n","\n","These are servers that mostly designed to host/access large multidimensional scientific datasets (e.g., NetCDF or HDF formats) but are not suitable for other formats like CSV or JSON. Some types of these servers are: THREDDS (Thematic Real-time Environmental Distributed Data Services); OPeNDAP (Open-source Project for a Network Data Access Protocol). These can be accessed via scientific tools/libraries (e.g., xarray, netCDF4, nccopy, Panoply) or protocols like OPeNDAP allow client-side access via URLs (wget). There is some flexibility for accessing subset of data but no functionality for filtering by metadata or aggregation.\n","\n","***Demonstration examples:***\n","\n","https://www.nccs.nasa.gov/services/data-collections/land-based-products/nex-gddp-cmip6\n","\n","https://ds.nccs.nasa.gov/thredds/catalog/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/catalog.html\n","\n","https://ds.nccs.nasa.gov/thredds/catalog/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/catalog.html?dataset=AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc\n"],"metadata":{"id":"pKkYiby6dqwl"}},{"cell_type":"markdown","source":["### (i) Downloading via command line"],"metadata":{"id":"J9CQMRuWZAwg"}},{"cell_type":"code","source":["# You can use the \"HTTPServer\" URL to directly download the while file without any subsetting\n","#!wget https://ds.nccs.nasa.gov/thredds/fileServer/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc\n","#!rm pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc"],"metadata":{"id":"lGJIpyQNZsXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can use the \"NetcdfSubset\" URL to directly download the file but you can also subset the data to your requirements\n","# Link here https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc/dataset.html\n","!wget -O test.nc \"https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc?var=pr&north=40&west=66&east=100&south=8&horizStride=1&time=1950-12-31T12:00:00Z&&accept=netcdf3\"\n","\n","# -O flag in wget allows you to rename the file you are downloading"],"metadata":{"id":"7JSfFW-bK06X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing packages\n","import numpy as np\n","import xarray as xr\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"O6pO2vX68SjL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the downloaded data\n","f = xr.open_dataset(\"test.nc\")\n","f['pr'][0].plot()"],"metadata":{"id":"2OhhZJEJQLfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Deleting the downloaded data\n","!rm test.nc"],"metadata":{"id":"hHDBPozL9SDG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (ii) Loading data into Python without downloading"],"metadata":{"id":"IejgRQlUbNk2"}},{"cell_type":"code","source":["# You can use the \"OpenDAP\" URL to directly load data into Python\n","# https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc.html\n","xr.open_dataset('https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc')"],"metadata":{"id":"dCt60n_UOb3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The OpenDAP servers provide the functionality to subset the data as needed\n","xr.open_dataset(\"https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc?lat[0:1:100],pr[0:1:0][0:1:0][0:1:0],time[0:1:10],lon[0:1:100]\")"],"metadata":{"id":"8PzzuzvTUJOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Or you can read the full data file and subset it using xarray\n","f = xr.open_dataset('https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc')\n","f.sel(lat=slice(8,40), lon=slice(66,100)).pr[0].plot()"],"metadata":{"id":"CQ8-ydqRcSdG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (iii) Reading in multiple files together"],"metadata":{"id":"BUTjTfoocnMW"}},{"cell_type":"code","source":["# Using the \"OpenDAP\" URLs for different files, you can read multiple files together without downloading\n","\n","# For this you would need to first create a list of the file URLs\n","base_url = 'https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_'\n","years = range(1950, 1952)  # Adjust the range as needed\n","\n","file_urls = [f\"{base_url}{year}_v1.1.nc\" for year in years]\n","file_urls"],"metadata":{"id":"Utd_Ifi-v9Sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Then use the list of URLs to open all of them directly using xarray function to open multiple files\n","f = xr.open_mfdataset(file_urls, combine='by_coords')\n","f.sel(lat=slice(8,40), lon=slice(66,100))"],"metadata":{"id":"sPY8709Cv9Pm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3.2. APIs (Application Programming Interfaces)\n","Now a days, many data providers offer RESTful APIs that allow users to query and retrieve data programmatically. APIs are powerful for accessing dynamic content, filtering by time, location, or variable, and automating data workflows. The filtering can easily be done using Python scripts.\n","\n","***Demonstration examples:***\n","\n","[Climate Data Store](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=overview)\n","\n","[COSMOS API](https://cosmos-api.ceh.ac.uk/docs)\n","\n","For the training session we will be accessing station observations from the [COSMOS-UK network](https://www.ceh.ac.uk/our-science/projects/cosmos-uk). We will be accessing observed variable of Daily Maximum Temperature Air (TA_MAX) from 2016--2022 for one of the COSMOS station, [Alice Holt (ALIC1)](https://cosmos.ceh.ac.uk/sites/ALIC1), directly from the API\n","![COSMOS_ALCI1.png](https://raw.githubusercontent.com/NERC-CEH/UKCEH_Summer_School/refs/heads/main/content/COSMOS_ALIC1.png)"],"metadata":{"id":"eQJdXn_Qbd2K"}},{"cell_type":"markdown","source":["### (i) Importing required packages"],"metadata":{"id":"7owZLsOsZFWd"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import xarray as xr\n","from datetime import datetime\n","import io\n","import json\n","import requests\n","import zipfile\n","import matplotlib.pyplot as plt"],"metadata":{"id":"v7dFUlYXEUaj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (ii) Pre-written functions"],"metadata":{"id":"NUZStadpZLnk"}},{"cell_type":"code","source":["# Pre-written functions are genereally give on the API webpage\n","# Here we are using these for accessing COSMOS data.\n","# Please see https://cosmos-api.ceh.ac.uk/python_examples for code examples\n","# Please see https://cosmos-api.ceh.ac.uk/docs for more details\n","\n","\n","def get_api_response(url, csv=False):\n","    \"\"\"Helper function to send request to API and get the response\n","\n","    :param str url: The URL of the API request\n","    :param bool csv: Whether this is a CSV request. Default False.\n","    :return: API response\n","    \"\"\"\n","    # Send request and read response\n","    print(url)\n","    response = requests.get(url)\n","\n","    if csv:\n","        return response\n","    else:\n","        # Decode from JSON to Python dictionary\n","        return json.loads(response.content)\n","\n","\n","def get_collection_parameter_info(params):\n","    \"\"\"A function for wrangling the collection information into a more visually appealing format!\"\"\"\n","    df = pd.DataFrame.from_dict(params)\n","    df = df.T[[\"label\", \"description\", \"unit\", \"sensorInfo\"]]\n","\n","    df[\"unit_symbol\"] = df[\"unit\"].apply(lambda x: x[\"symbol\"][\"value\"])\n","    df[\"unit_label\"] = df[\"unit\"].apply(lambda x: x[\"label\"])\n","    df[\"sensor_depth\"] = df[\"sensorInfo\"].apply(\n","        lambda x: None if pd.isna(x) else x[\"sensor_depth\"][\"value\"]\n","    )\n","\n","    df = df.drop([\"sensorInfo\", \"unit\"], axis=1)\n","\n","    return df\n","\n","\n","def format_datetime(dt):\n","    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","\n","\n","def read_json_collection_data(json_response):\n","    \"\"\"Wrangle the response JSON from a COSMOS-API data collection request into a more usable format - in this case a Pandas Dataframe\n","\n","    :param dict json_response: The JSON response dictionary returned from a COSMOS-API data collection request\n","    :return: Dataframe of data\n","    :rtype: pd.DataFrame\n","    \"\"\"\n","    # The response is a list of dictionaries, one for each requested site\n","\n","    # You can choose how you want to build your dataframes.  Here, I'm just loading all stations into one big dataframe.\n","    # But you could modify this for your own use cases.  For example you might want to build a dictionary of {site_id: dataframe}\n","    # to keep site data separate, etc.\n","    master_df = pd.DataFrame()\n","\n","    for site_data in resp[\"coverages\"]:\n","        # Read the site ID\n","        site_id = site_data[\"dct:identifier\"]\n","\n","        # Read the time stamps of each data point\n","        time_values = pd.DatetimeIndex(site_data[\"domain\"][\"axes\"][\"t\"][\"values\"])\n","\n","        # Now read the values for each requested parameter at each of the time stamps\n","        param_values = {\n","            param_name: param_data[\"values\"]\n","            for param_name, param_data in site_data[\"ranges\"].items()\n","        }\n","\n","        # And put everything into a dataframe\n","        site_df = pd.DataFrame.from_dict(param_values)\n","        site_df[\"datetime\"] = time_values\n","        site_df[\"site_id\"] = site_id\n","\n","        site_df = site_df.set_index([\"datetime\", \"site_id\"])\n","        master_df = pd.concat([master_df, site_df])\n","\n","    return master_df"],"metadata":{"id":"c3wRlM6mEUXX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (iii) Accessing Station Observations"],"metadata":{"id":"3Bme70OYZaqK"}},{"cell_type":"code","source":["# We need to extract \"ta_max\" parameter for COSMOS station \"ALIC1\" over the period of 2016 -- 2022\n","start_date = format_datetime(datetime(2016, 1, 1))\n","end_date = format_datetime(datetime(2022, 12, 31))\n","query_date_range = f\"{start_date}/{end_date}\"\n","param_name = [\n","    \"ta_max\",\n","]\n","site_nm = \"ALIC1\""],"metadata":{"id":"jM43n87TEUUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First we get the metadata for the COSMOS station\n","BASE_URL = \"https://cosmos-api.ceh.ac.uk\"\n","site_info_url = f\"{BASE_URL}/collections/1D/locations\"\n","site_info_response = get_api_response(site_info_url)\n","\n","site_info = {}\n","for site in site_info_response[\"features\"]:\n","    site_id = site[\"id\"]\n","    site_name = site[\"properties\"][\"label\"]\n","    coordinates = site[\"geometry\"][\"coordinates\"]\n","    date_range = site[\"properties\"][\"datetime\"]\n","    start_date, end_date = date_range.split(\"/\")\n","\n","    other_info = site[\"properties\"][\"siteInfo\"]\n","    other_info = {key: d[\"value\"] for key, d in other_info.items()}\n","\n","    site_info[site_id] = {\n","        \"site_name\": site_name,\n","        \"coordinates\": coordinates,\n","        \"start_date\": start_date,\n","        \"end_date\": end_date,\n","    } | other_info\n","\n","site_info_df = pd.DataFrame.from_dict(site_info).T\n","s_df = site_info_df[site_info_df.index == site_nm]\n","s_df"],"metadata":{"id":"L4HGK6B7EURm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting the COSMOS station latitude and longitude from the whole metadata list\n","# COSMOS station latitude and longitude is required to calculate the nearest grid point on the CHESS grid to extract corresponding model data\n","site_latitude = s_df[\"coordinates\"][0][0]\n","site_longitude = s_df[\"coordinates\"][0][1]\n","print(\n","    \"COMOS Site \"\n","    + site_nm\n","    + \" Latitude: \"\n","    + str(site_latitude)\n","    + \" Longitude: \"\n","    + str(site_longitude)\n",")"],"metadata":{"id":"fiZv78dhEUOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting COSMOS TA_MAX data for the station over the required period into a pandas dataframe\n","query_url = f'{BASE_URL}/collections/1D/locations/{site_nm}?datetime={query_date_range}&parameter-name={\",\".join(param_name)}'\n","resp = get_api_response(query_url)\n","df = read_json_collection_data(resp)\n","df = df.reset_index()\n","display(df)\n","print(df.shape)"],"metadata":{"id":"2db5TQ-xEULO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating monthly climatological values of TA_MAX for the station over 2016--2022\n","df_site = (\n","    df.groupby(pd.PeriodIndex(df[\"datetime\"], freq=\"M\"))[\"ta_max\"].mean().reset_index()\n",")\n","df_site[\"datetime\"] = df_site.datetime.dt.to_timestamp()\n","df_site = df_site.groupby(df_site[\"datetime\"].dt.month).mean(\"ta_max\")\n","df_site"],"metadata":{"id":"fNrKOqRQEUIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3.3. Cloud Storage Access\n","\n","Many datasets are now being hosted in cloud storages, especially for large-scale earth observation data. Tools like AWS CLI, boto3 (Python), gsutil, or cloud-native file systems (e.g., s3fs, gcsfs) allow seamless access to cloud-hosted data. Examples are AWS S3, Google Cloud Storage, Azure.\n","\n","![Object_Store.png](https://raw.githubusercontent.com/NERC-CEH/UKCEH_Summer_School/refs/heads/main/content/Object_Store.png)\n","\n","\n","**OTHER RESOURCES:**\n","1. Please see the [GitHub respository](https://github.com/NERC-CEH/object_store_tutorial) with a guide on utilizing object storage.\n","2. Please see the video to hear more about [JASMIN Object Storage: Optimizing Performance for Climate Research](https://www.youtube.com/watch?v=xJ8qEXQAri0&list=PLhF74YhqhjqnXvjzFCKnw4TGAFnkVu7Qn&index=2) from the JASMIN User Conference 2023."],"metadata":{"id":"XF5OxaEObkIr"}},{"cell_type":"markdown","source":["### (i) Importing required packages and pre-written functions"],"metadata":{"id":"mBB5uji5ZmME"}},{"cell_type":"code","source":["import fsspec\n","import s3fs\n","import zarr\n","import xarray as xr"],"metadata":{"id":"kYm8v1AtSDCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def open_zarr_from_s3(endpoint_url: str,\n","                      store_path: str):\n","    \"\"\"\n","    Open a Zarr dataset hosted on an S3‑compatible object store.\n","\n","    Parameters\n","    ----------\n","    endpoint_url : str\n","        Base S3 endpoint, e.g. \"https://chess-scape-o.s3-ext.jc.rl.ac.uk\".\n","    store_path : str\n","        Path to the Zarr store inside the bucket, e.g.\n","        \"ens01-year100kmchunk/tmax_01_year100km.zarr\".\n","\n","    Returns\n","    -------\n","    xr.Dataset\n","        The opened Zarr dataset.\n","    \"\"\"\n","    # 1. Create an fsspec filesystem for the S3 endpoint\n","    fs = fsspec.filesystem(\"s3\", asynchronous=True, anon=True,\n","                           endpoint_url=endpoint_url,)\n","\n","    # 2. Wrap it in a Zarr store\n","    zstore = zarr.storage.FsspecStore(fs, path=store_path)\n","\n","    # 3. Open the dataset with xarray\n","    ds = xr.open_zarr(zstore, decode_times=True, decode_cf=True)\n","\n","    return ds"],"metadata":{"id":"bCz-o1bMvTsD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (ii) Exploring the data in Object Store"],"metadata":{"id":"6CYibmllZ0w7"}},{"cell_type":"code","source":["# JASMIN Object Store tenancy we are using is chess-scape-o, the URL is as follows\n","jasmin_s3_url = \"https://chess-scape-o.s3-ext.jc.rl.ac.uk\""],"metadata":{"id":"P7OXY_V4wROQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# s3fs is a python package that allows you to not only read the data but also explore the tenancy ()\n","# Here we will be using s3fs to list the bucket and not read the data, we read the data using intake package shown below\n","# For more information please see: https://pypi.org/project/s3fs/\n","s3 = s3fs.S3FileSystem(anon=True, client_kwargs={'endpoint_url': jasmin_s3_url})\n","s3.ls('s3://ens01-year100kmchunk/')\n","\n","# In the output you see that within in the chess-scape-o tenancy, a bucket called ens01-year100kmchunk\n","# has 10 different zarr files for different 10 different variables. This is for a single chunk type tested."],"metadata":{"id":"rbA2bkZaYFgw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (iii) Accessing data and the associated metadata"],"metadata":{"id":"EufRj92yaClD"}},{"cell_type":"code","source":["# We are accessing TASMAX for the ensemble member #01 from the catalogue\n","chess_data_01 = open_zarr_from_s3(jasmin_s3_url, \"ens01-year100kmchunk/tmax_01_year100km.zarr\")\n","chess_data_01"],"metadata":{"id":"q-i_HtPswASI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CHESS-SCAPE is on the British National Grid with Easting and Northing Coordinates.\n","# We also set the latitude and longitude as coordinates\n","chess_data_01 = chess_data_01.set_coords((\"lat\", \"lon\"))\n","chess_data_01"],"metadata":{"id":"LhmWJ5udTvn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Slicing for the time period 2016--2022\n","chess_data_01 = chess_data_01[\"tasmax\"].sel(time=slice(\"2016-01-01\", \"2022-12-30\"))\n","chess_data_01"],"metadata":{"id":"_Mld7VVgSC-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# Extracting data for the other ensemble members\n","# Ensemble member #04\n","chess_data_04 = open_zarr_from_s3(jasmin_s3_url, \"ens04-year100kmchunk/tmax_04_year100km.zarr\")\n","chess_data_04 = chess_data_04.set_coords((\"lat\", \"lon\"))\n","chess_data_04 = chess_data_04[\"tasmax\"].sel(time=slice(\"2016-01-01\", \"2022-12-30\"))\n","\n","# Ensemble member #06\n","chess_data_06 = open_zarr_from_s3(jasmin_s3_url, \"ens06-year100kmchunk/tmax_06_year100km.zarr\")\n","chess_data_06 = chess_data_06.set_coords((\"lat\", \"lon\"))\n","chess_data_06 = chess_data_06[\"tasmax\"].sel(time=slice(\"2016-01-01\", \"2022-12-30\"))\n","\n","# Ensemble member #15\n","chess_data_15 = open_zarr_from_s3(jasmin_s3_url, \"ens15-year100kmchunk/tmax_15_year100km.zarr\")\n","chess_data_15 = chess_data_15.set_coords((\"lat\", \"lon\"))\n","chess_data_15 = chess_data_15[\"tasmax\"].sel(time=slice(\"2016-01-01\", \"2022-12-30\"))"],"metadata":{"id":"cATiwhH9SC65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (iv) Deriving the Observed Station nearest grid point on the Gridded Dataset"],"metadata":{"id":"GnWy_oxtVo79"}},{"cell_type":"code","source":["# Function to derive the data for the nearest grid point to the station lat lon\n","def find_chess_tile(lat, lon, latlon_ref):\n","    \"\"\"\n","    Created by Doran Khamis (dorkha@ceh.ac.uk)\n","    Function to calculate the nearest grid point\n","    of a given lat lon value within a gridded dataset\n","    The input data is the latitude, longitude of the station\n","    and the grid reference (latlon_ref) of the gridded dataset\n","    The function returns the y and x index for the gridded dataset\n","    which can be used to derive the nearest grid point\n","    This function assumes equal length lat/lon vectors in latlon_ref\n","    \"\"\"\n","    dist_diff = np.sqrt(\n","        np.square(latlon_ref.lat.values - lat) + np.square(latlon_ref.lon.values - lon)\n","    )\n","    chesstile_yx = np.where(dist_diff == np.min(dist_diff))\n","    return chesstile_yx"],"metadata":{"id":"pEjOZ3qrVo7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We create a temporary CHESS-SCAPE gridded dataset array\n","chess_tmp = chess_data_01[0, :, :]\n","chess_tmp"],"metadata":{"id":"TXKFPzSaVo7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting the x and y indices which point to the nearest grid point of the COSMOS station\n","y, x = find_chess_tile(site_latitude, site_longitude, chess_tmp)\n","print(y,x)"],"metadata":{"id":"SoDW11UwVo7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Deleting the temporary array\n","del chess_tmp"],"metadata":{"id":"JEZmSVf4Vo7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (v) Extracting the model ensemble data for the grid point nearest to the observed station"],"metadata":{"id":"Cl62qOTWWE9z"}},{"cell_type":"code","source":["# Creating arrays for day, month and year from the time index\n","day = np.array([i.day for i in chess_data_01.time.values])\n","month = np.array([i.month for i in chess_data_01.time.values])\n","year = np.array([i.year for i in chess_data_01.time.values])"],"metadata":{"id":"FSiMAa2nWE9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Indexing the CHESS-SCAPE data with the x and y coordinates nearest to the observed station\n","ens = [\"ENS01\", \"ENS04\", \"ENS06\", \"ENS15\"]\n","chess_site_data = np.zeros((len(ens), len(day)))\n","chess_site_data[0, :] = chess_data_01[:, y, x].squeeze().values\n","chess_site_data[1, :] = chess_data_04[:, y, x].squeeze().values\n","chess_site_data[2, :] = chess_data_06[:, y, x].squeeze().values\n","chess_site_data[3, :] = chess_data_15[:, y, x].squeeze().values"],"metadata":{"id":"8-bjGFzDWE90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting CHESS-SCAPE temperature from Kelvin to deg Celsius\n","chess_site_data = chess_site_data - 273.15"],"metadata":{"id":"Gq-teJ7LWE90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a pandas dataframe for CHESS-SCAPE ensemble TASMAX\n","f = np.vstack((year, month, day, chess_site_data))\n","df = pd.DataFrame(f.T, columns=[\"YEAR\", \"MONTH\", \"DAY\"] + ens)\n","df"],"metadata":{"id":"Bx4xH13PWE90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating monthly climatology of TASMAX for all the ensemble members\n","df_model = df.groupby([\"YEAR\", \"MONTH\"])[ens].mean()\n","df_model = df_model.groupby([\"MONTH\"])[ens].mean()\n","df_model"],"metadata":{"id":"1wOPCVM7WE90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (vi) Comparing observations against modelled ensemble projection"],"metadata":{"id":"1dtgmKSMWE90"}},{"cell_type":"code","source":["# List of months\n","months = [\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\",\n","          \"JUL\", \"AUG\", \"SEP\", \"OCT\", \"NOV\", \"DEC\",]"],"metadata":{"id":"ZsEPAStCWE90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating model ensemble mean, minimum and maximum\n","df_model_max = df_model.max(axis=1)\n","df_model_min = df_model.min(axis=1)\n","df_model_mn = df_model.mean(axis=1)"],"metadata":{"id":"HUDyAMbZWE90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting monthly climatology of Daily Maximum Air Temperature from COSMOS station ALIC1 and nearest grid point on CHESS-SCAPE averaged over 2016--2022\n","fig = plt.figure(figsize=(10, 6))\n","plt.plot(months, df_site.values, color=\"k\", lw=3, label=\"OBSERVED\")\n","plt.plot(months, df_model_mn.values, color=\"b\", ls=\"--\", lw=2, label=\"MODEL MEAN\")\n","plt.fill_between(\n","    months,\n","    df_model_min.values,\n","    df_model_max.values,\n","    color=\"b\",\n","    alpha=0.3,\n","    label=\"MODEL SPREAD\",\n",")\n","plt.ylabel(\"Daily Maximum Air Temperature ($^\\circ$C)\", fontsize=15)\n","plt.yticks(np.arange(7, 26, 2), fontsize=15)\n","plt.xticks(fontsize=15)\n","plt.legend(loc=\"upper left\", fontsize=15)\n","plt.title(site_nm + \" - Monthly Climatology 2016 - 2022\", fontsize=20)\n","plt.show()"],"metadata":{"id":"Li_rtfkDWE90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3.4. Others\n","\n","There are several other ways to access data remotely, and increasingly, most of these methods are being supported by cloud-based infrastructure.  \n","\n","  - Emerging cloud-native standards, such as the Spatiotemporal Asset Catalog (STAC), enable efficient cataloging and discovery of satellite or gridded climate data across distributed systems. These are often integrated with cloud platforms, allowing users to search and access data programmatically with minimal overhead. For example, [ECMWF Data Stores STAC Catalogue API](https://cds.climate.copernicus.eu/stac-browser/?.language=en).\n","  - Sensor Observation Services (SOS) are also available and are particularly useful for accessing real-time or near-real-time observations, especially from in-situ sensor networks or environmental monitoring platforms. For example, [UK AIR](https://uk-air.defra.gov.uk/data/data-availability). In the near future, the [FDRI project](https://fdri.org.uk/) will provide monitoring data covering the whole hydrological system.\n","  - Finally, platforms such as [Google Earth Engine (GEE)](https://developers.google.com/earth-engine/datasets) and other open data cube frameworks enable users to query and analyze massive gridded datasets remotely, leveraging built-in computational resources—eliminating the need to download data locally. The GEE platform offers [comprehensive tutorials](https://developers.google.com/earth-engine/guides/getstarted) that guide users through its functionality. As a self-learning exercise, we have included a GitHub repository developed at UKCEH, linked in this workshop’s directory. It provides step-by-step training on [extracting drought indicators using GEE in Python](https://github.com/eugmag/Google_Earth_Engine_python_demo/tree/main), allowing you to follow along at your own pace."],"metadata":{"id":"euMABLX0DYyI"}},{"cell_type":"markdown","source":["# Thank you! Come talk to us about what sort of data are you looking to download and/or access?"],"metadata":{"id":"smOBw4UxAiI8"}}]}

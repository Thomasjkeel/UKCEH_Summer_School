{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1750773638069,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "bpE0FlM9-bGY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install s3fs zarr cftime fsspec rioxarray folium mapclassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -O data_utils.py https://raw.githubusercontent.com/NERC-CEH/UKCEH_Summer_School/refs/heads/main/Workshop_4/data_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ijGhr8x-YmE"
   },
   "source": [
    "*Workshop 4. Doing research with hydrological data*\n",
    "\n",
    "\n",
    "# Practical: Comparison of Rainfall Data: Gauges vs Gridded Estimates\n",
    "\n",
    "\n",
    "## Learning objectives:\n",
    "- Understand how to read in, format and analyse rainfall data from different sources\n",
    "- Calculate and interpret basic statistics to compare gridded rainfall estimates and rain gauge observations \n",
    "- Learn how to test and scale up research code\n",
    "\n",
    "# Introduction\n",
    "Rainfall is notoriously difficult to measure accurately, but in the UK we have a dense network of both [rain gauges](https://en.wikipedia.org/wiki/Rain_gauge) and [rain radar](https://en.wikipedia.org/wiki/Weather_radar) stations (**Figure 1**). These network gives us a relatively reliable and consistent estimate of rainfall through time, even when there is missing or wrong data, because estimates are cross-checked agaisnt other nearby measurements. \n",
    "\n",
    "![Map of UK rain gauge stations](https://raw.githubusercontent.com/NERC-CEH/UKCEH_Summer_School/refs/heads/main/content/Maps-of-daily-rain-gauges-used-to-derive-the-CEH-GEAR-data-set-a-monthly-rain-gauges.png)  \n",
    "*Figure 1. Maps of daily rain gauges in 2012: (a) monthly rain gauges and (b) daily rain gauges (source: Keller et al. 2015).*\n",
    "\n",
    "\n",
    "The UKCEH is also working-on a updated interactive map of the UK's rain gauge network (as of 2025), which you can find a version of [here](https://thomasjkeel.github.io/UK-Rain-Gauge-Network/gauges.html).\n",
    "\n",
    "## Gridded rainfall data\n",
    "> *Gridded rainfall products provide a spatially- and temporally-uniform estimate of rainfall*\n",
    "\n",
    "Whilst, the UK's rain gauge network provides valuable data, its coverage is both spatially and temporally uneven. Some areas—such as Scotland—have a much denser concentration of gauges than others, like the South of England (see Figure 1). Additionally, the gauges themselves vary in age and data quality.  \n",
    "\n",
    "As such, it can be inconvenient to work with rain gauge data. For this reason, environmental researchers often prefer datasets that are interpolated onto regular grids, which are easier to process, analyze, and compare over space and time.\n",
    "\n",
    "\n",
    "For the UK, two main gridded rainfall products exist:  \n",
    "1. CEH-GEAR — provides daily/monthly 1km gridded rainfall, produced by the UKCEH and described in [Keller et al. 2015](https://essd.copernicus.org/articles/7/143/2015/)\n",
    "2. HadUK-Grid — provides daily/monthly/seasonal/annual 1km gridded rainfall, produced by the Met Office and described in [Hollis et al. 2019](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/gdj3.78)\n",
    "\n",
    "Each product provides slightly different methodology for spatially interpolating data from the UK rain gauge network to a regular 1km grid, shown diagramatically in Figure 2 (see more in those papers if you are interested in the specifics). For this practical, we will be using the CEH-GEAR dataset, but for future, you can access the HadUK-Grid [here](https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb/)).\n",
    "\n",
    "\n",
    "![example of gridding](https://raw.githubusercontent.com/NERC-CEH/UKCEH_Summer_School/refs/heads/main/content/gridding_example_from_gdal.png)  \n",
    "*Figure 2. Diagram showing example of interpolation of points to regular grid (source: GDAL 2025).*\n",
    "\n",
    "### Onto the practical\n",
    "\n",
    "In this practical, we will read in both gridded rainfall and rain gauge data and compare them.  \n",
    "To load data, we will use the following libraries...\n",
    "- Polars (a faster version of Pandas)\n",
    "- xarray (to read in NetCDF and TIF files)\n",
    "- Geopandas (to read shapefiles)\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"https://raw.githubusercontent.com/Thomasjkeel/UKCEH_Summer_School/refs/heads/main/content/polars_logo_icon_248809.png\" alt=\"polars-logo\" style=\"width:200px;\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Thomasjkeel/UKCEH_Summer_School/refs/heads/main/content/geopandas-logo.png\" alt=\"geopandas-logo\" style=\"width:300px;\"/>\n",
    "    <img src=\"https://raw.githubusercontent.com/Thomasjkeel/UKCEH_Summer_School/refs/heads/main/content/xarray-logo.png\" alt=\"xarray-logo\" style=\"width:200px;\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1750777762706,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "lc_cCM8wC78K"
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import fsspec\n",
    "import zarr\n",
    "import rioxarray\n",
    "\n",
    "import geopandas as gpd\n",
    "import polars as pl\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry\n",
    "\n",
    "# practical-specific file:\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for coordinates of Severn River catchment\n",
    "SEVERN_NORTHING_RANGE = [270000, 349000]\n",
    "SEVERN_EASTING_RANGE = [280000, 390000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUk-xBYEHQTr"
   },
   "source": [
    "# 1. Load rainfall data\n",
    "First we will read in gridded rainfall and rain gauge data using code similar to that shown in Workshop 2.\n",
    "\n",
    "**External data sources:**\n",
    "- UK-wide CEH-GEAR rainfall data - from the [JASMIN object-store](https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store/)\n",
    "- Rain gauge data for the Severn River Catchment - from the [JASMIN object-store](https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store/)\n",
    "- Severn catchment boundaries - from the National River Flow Archive ([NRFA](https://nrfa.ceh.ac.uk/data/search))\n",
    "- River Severn watercourse - from [OS Open Rivers](https://www.ordnancesurvey.co.uk/products/os-open-rivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load CEH-GEAR (gridded rainfall data)\n",
    "Using the code from Workshop 2, below we connect to the S3 bucket on the JASMIN object-store (remote) and load in a local version of the daily CEH-GEAR file (\"geardaily_fulloutput_yearly_100km_chunks.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1750773656615,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "caSI-CEpHQED",
    "outputId": "f7e0a7ec-80f6-4884-fd52-12e62f3299d8"
   },
   "outputs": [],
   "source": [
    "fdri_fs = fsspec.filesystem(\n",
    "    \"s3\", asynchronous=True, anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\"\n",
    ")\n",
    "gear_daily_zstore = zarr.storage.FsspecStore(\n",
    "    fdri_fs, path=\"geardaily/GB/geardaily_fulloutput_yearly_100km_chunks.zarr\"\n",
    ")\n",
    "gear_daily = xr.open_zarr(gear_daily_zstore, decode_times=True, decode_cf=True)\n",
    "gear_daily  # 310 GB worth of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these practicals, there will be questions, please fill them in the best you can before moving on (or ask us for help)\n",
    "##### ❓ Question: what type of file is the input data?\n",
    "Replace the ??? in this markdown box with your answer \n",
    "(*hint: check the file extension*)\n",
    "\n",
    "**Answer:** ???\n",
    "\n",
    "##### ❓ Question: what data type is the 'gear_daily' data we have loaded in?\n",
    "\n",
    "Replace the ??? in the code-block below with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at one day in the CEH-GEAR dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data \n",
    "# may take about 50 seconds\n",
    "fig, ax = plt.subplots(1)\n",
    "gear_daily.sel(time=\"2013-02-13\")[\"rainfall_amount\"].plot(ax=ax, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, this figure shown daily rainfall for the entire UK from CEH-GEAR product. The CEH-GEAR spatially interpolates rain gauge observations to a regular 1km by 1km grid\n",
    "\n",
    "## 1.2 Load daily rain gauge data\n",
    "Next, we will load daily rain gauge data and it's associated metadata from the JASMIN object-store.\n",
    "\n",
    "🐻‍❄️ **Note:** we use the [polars](https://pola.rs/) instead of pandas to load data, but its syntax should be relatively familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2869,
     "status": "ok",
     "timestamp": 1750773231213,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "3aDZX8AoAkLY"
   },
   "outputs": [],
   "source": [
    "severn_rain_gauge_data = pl.read_csv(\n",
    "    \"s3://rain-gauge/hourly_severn_rain_gauge_data.csv\",\n",
    "    storage_options={\"endpoint_url\": \"https://fdri-o.s3-ext.jc.rl.ac.uk\", \"anon\": True},\n",
    "    try_parse_dates=True,\n",
    ")\n",
    "severn_rain_gauge_metadata = pl.read_csv(\n",
    "    \"s3://rain-gauge/hourly_severn_rain_gauge_metadata.csv\",\n",
    "    storage_options={\"endpoint_url\": \"https://fdri-o.s3-ext.jc.rl.ac.uk\", \"anon\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1750774478219,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "mby4EYPLAmym",
    "outputId": "cba5df9e-859a-42cf-a2fa-567ae3d493ad"
   },
   "outputs": [],
   "source": [
    "severn_rain_gauge_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Question: what type of file is the input data?\n",
    "\n",
    "**Answer:** ???\n",
    "\n",
    "##### ❓ Question: what data type is the 'severn_rain_gauge_data'?\n",
    "\n",
    "Replace the ??? in the code-block below with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subset the data to look at precipitation from a single rain gauge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_data.filter(pl.col(\"ID\") == 90537)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🐼: above code is equivalent to: `severn_rain_gauge_data.loc[severn_rain_gauge_data[\"ID\"] == 90537]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc0XNt7dAz5V"
   },
   "source": [
    "#### 🤨 Task: Subset the precipitation data to the station: \"WALFORD\" \n",
    "\n",
    "Replace the ??? below with your answer\n",
    "\n",
    "*Hint: you can use the `.filter` method like above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Find the \"ID\" for the station with the name: \"WALFORD\" in the metadata\n",
    "severn_rain_gauge_metadata.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Using that ID, subset the rain gauge data\n",
    "severn_rain_gauge_data.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load spatial datasets for Upper Severn catchment\n",
    "Finally, we download some spatial data for the River Severn, including catchment boundaries from the NRFA, and watercourse data from Ordinance Survey.\n",
    "\n",
    "Data originally downloaded from: [NRFA](https://nrfa.ceh.ac.uk/data/search) & [Ordinance Survey](https://www.ordnancesurvey.co.uk/products/os-open-rivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bash script below will fetch and download spatial data from GitHub and store it locally in the folder 'severn_catchment_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://raw.githubusercontent.com/NERC-CEH/UKCEH_Summer_School/refs/heads/main/Workshop_4/severn_catchment_data.tar.gz\n",
    "!mkdir -p severn_catchment_data\n",
    "!tar -xvf severn_catchment_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load shapefiles & geojson using geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greater Severn catchment boundary\n",
    "bewdley_shp = gpd.read_file(\"severn_catchment_data/Bewdley/54001/54001.shp\")\n",
    "\n",
    "# Upper Severn catchment boundaries\n",
    "abermule_shp = gpd.read_file(\"severn_catchment_data/Abermule/54014/54014.shp\")\n",
    "dolwen_shp = gpd.read_file(\"severn_catchment_data/Dolwen/54080/54080.shp\")\n",
    "plynlimon_shp = gpd.read_file(\"severn_catchment_data/Plynlimon Flume/54022/54022.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rivers linestrings\n",
    "severn_catchment_river_linestrings = gpd.read_file(\n",
    "    \"severn_catchment_data/rivers_around_severn.geojson\"\n",
    ")\n",
    "severn_river_linestrings = severn_catchment_river_linestrings.loc[\n",
    "    severn_catchment_river_linestrings[\"name1\"].str.contains(\"River Severn\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the catchment boundaries and the River Severn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6), sharex=True, sharey=True)\n",
    "ax.set_xlabel(\"Easting\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "abermule_shp.plot(ax=ax, facecolor=\"none\", alpha=0.5, linewidth=0.5)\n",
    "bewdley_shp.plot(ax=ax, facecolor=\"none\", alpha=0.5, linewidth=0.5)\n",
    "dolwen_shp.plot(ax=ax, facecolor=\"none\", alpha=0.5, linewidth=0.5)\n",
    "plynlimon_shp.plot(ax=ax, facecolor=\"none\", alpha=0.5, linewidth=0.5)\n",
    "\n",
    "severn_catchment_river_linestrings.plot(ax=ax, alpha=0.1)\n",
    "severn_river_linestrings.plot(ax=ax, alpha=0.6)\n",
    "ax.set_title(\"Upper parts of the River Severn\")\n",
    "plt.subplots_adjust(hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load in some height data and plot that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_hght = rioxarray.open_rasterio(\"severn_catchment_data/HGHT_SEVERN_1KM_CLIP.tif\")\n",
    "severn_hght = severn_hght.sortby(\"y\")\n",
    "severn_hght = severn_hght.sel(band=1)\n",
    "severn_hght = severn_hght / 10  # divide by 10, so in metres\n",
    "severn_hght = severn_hght.sel(\n",
    "    x=slice(*SEVERN_EASTING_RANGE), y=slice(*SEVERN_NORTHING_RANGE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6), sharex=True, sharey=True)\n",
    "ax.set_xlabel(\"Easting\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "abermule_shp.plot(ax=ax, facecolor=\"none\", linewidth=0.5)\n",
    "bewdley_shp.plot(ax=ax, facecolor=\"none\", linewidth=0.5)\n",
    "dolwen_shp.plot(ax=ax, facecolor=\"none\", linewidth=0.5)\n",
    "plynlimon_shp.plot(ax=ax, facecolor=\"none\", linewidth=0.5)\n",
    "severn_hght.plot(ax=ax, alpha=0.7, cbar_kwargs={\"label\": \"height (m)\"})\n",
    "\n",
    "ax.set_title(\"Height profile of upper parts of the River Severn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjOTDCX1Avu_"
   },
   "source": [
    "# 2. Format the data\n",
    "Most of data science is about data cleaning and formatting. This is especially true for those using environmental data, where we often have datasets from many different sources, and that are of varying quality.\n",
    "\n",
    "In this section, we will prepare the datasets, so we can compare the rain gauge dataset to gridded rainfall in the next section.\n",
    "\n",
    "\n",
    "## 2.1 Checking for gaps in rain observation data\n",
    "There are many issues with the rain gauge observation dataset we are using because it has not been quality controlled. This includes gaps, streaks of repeating values, exceedances of world records, misinputted data, etc.. However, in the interest of moving swiftly on, we will only focus on finding gaps in this practical. To do this we calculate a data 'completeness' for each gauge comparing number of rows to expected number of rows.\n",
    "\n",
    ">*Note:* if you are interested in quality controlling rain gauge data, see the Python package [RainfallQC](https://github.com/NERC-CEH/RainfallQC) and find some examples [here](https://github.com/Thomasjkeel/RainfallQC-notebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save RAM later on, we'll filter out rows before 1990\n",
    "severn_rain_gauge_data = severn_rain_gauge_data.filter(\n",
    "    pl.col(\"DATETIME\") > pl.date(year=1990, month=1, day=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1750775769202,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "iGWSxOU4AvMG",
    "outputId": "9e0aedf8-568c-4b7f-81c1-4e996fff337d"
   },
   "outputs": [],
   "source": [
    "# load in a single gappy gauge\n",
    "one_gauge = severn_rain_gauge_data.filter(pl.col(\"ID\") == 90806)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data to see gaps\n",
    "one_gauge.to_pandas().set_index(\"DATETIME\")[\n",
    "    \"PRECIPITATION\"\n",
    "].plot()  # here we convert polars to pandas to let us plot longer time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, clearly there are some gaps in the data. We do not want to include this in our final clean dataset.  \n",
    "\n",
    "Let's create a completeness metric, by comparing the start and end dates of each gauge, and then checking how many data rows are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed number of days between start and end date (i.e. this is the number of observations in the data)\n",
    "severn_rain_gauge_data_observed = severn_rain_gauge_data.group_by(\"ID\").agg(\n",
    "    [\n",
    "        pl.col(\"DATETIME\").min().alias(\"start_date\"),\n",
    "        pl.col(\"DATETIME\").max().alias(\"end_date\"),\n",
    "        pl.len().alias(\"observed_days\"),\n",
    "    ]\n",
    ")\n",
    "severn_rain_gauge_data_observed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expected number of days between start and end date\n",
    "severn_rain_gauge_data_observed_and_expected = (\n",
    "    severn_rain_gauge_data_observed.with_columns(\n",
    "        [\n",
    "            ((pl.col(\"end_date\") - pl.col(\"start_date\")).dt.total_days() + 1).alias(\n",
    "                \"expected_days\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "severn_rain_gauge_data_observed_and_expected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide observed by expected and get percentage completeness\n",
    "severn_rain_gauge_data_completeness = (\n",
    "    severn_rain_gauge_data_observed_and_expected.with_columns(\n",
    "        [\n",
    "            ((pl.col(\"observed_days\") / pl.col(\"expected_days\")) * 100).alias(\n",
    "                \"perc_completeness\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "severn_rain_gauge_data_completeness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the rain gauges are particularly gappy...  \n",
    "Remember 60 % completeness, means that only 60% of the days between the start and end date actually have a record for rainfall (maybe someone forgot on the other days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_data_completeness.filter(pl.col(\"perc_completeness\") < 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always a good idea to use a histogram to view the distribution of your data too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "sns.histplot(severn_rain_gauge_data_completeness[\"perc_completeness\"], ax=ax)\n",
    "ax.set_ylabel(\"Number of gauges\")\n",
    "ax.set_xlabel(\"Completeness (%)\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, there are only a few gauges with very gappy data, let's remove everything below 90% just to be safe\n",
    "\n",
    "### 🤨 Task: Filter out data with less than 90% completeness\n",
    "Let's create a dataset that is more reliable\n",
    "\n",
    "Replace the ??? below with your answer\n",
    "\n",
    "*Hint: remember to break down the problem into steps, and print things out in a new code block if unsure*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Get the IDs of gauges above 90% threshold\n",
    "reliable_gauge_ids = severn_rain_gauge_data_completeness.filter(\n",
    "    pl.col(???) > ???\n",
    ")[\"ID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data to include only those ID's with more than 90% completeness\n",
    "severn_rain_gauge_data_filtered = severn_rain_gauge_data.???(\n",
    "    pl.col(\"ID\").is_in(???)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤨 Optional task: Take a closer look at the gauge 428554, what other issues can you see that will need to be quality controlled?:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Subset spatial data\n",
    "\n",
    "We have data for the entire UK, let's subset our data into the areas around the River Severn only and for data since 1990 (to make this notebook run quicker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset CEH-GEAR to SEVERN CATCHMENT\n",
    "severn_gear_daily = gear_daily.sel(\n",
    "    x=slice(*SEVERN_EASTING_RANGE),\n",
    "    y=slice(\n",
    "        *sorted(SEVERN_NORTHING_RANGE, reverse=True)\n",
    "    ),  # has to be reversed as y coordinates is descending\n",
    "    time=slice(\"1990\", None),\n",
    ")\n",
    "severn_gear_daily = severn_gear_daily.drop_vars(\"min_dist\")  # drop unecessary variable\n",
    "severn_gear_daily = severn_gear_daily.sortby(\"y\")  # flip y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load data into RAM (memory)\n",
    "# this code-block takes about 45 seconds\n",
    "severn_gear_daily.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the spatial profile of the rain gauge data in the below plot. There are huge gaps towards the east of the Bewdley catchment, but we will ignore these in this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.set_xlabel(\"Easting\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "abermule_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "bewdley_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "dolwen_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "plynlimon_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "sns.scatterplot(\n",
    "    x=\"EASTING\",\n",
    "    y=\"NORTHING\",\n",
    "    data=severn_rain_gauge_metadata.filter(pl.col(\"ID\").is_in(reliable_gauge_ids)),\n",
    ")\n",
    "ax.set_title(\"Distribution of rain gauges in Upper Severn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uqQ7OGAda96"
   },
   "source": [
    "# 3. Compare gridded rainfall product (CEH-GEAR) with rain gauge data\n",
    "Okay, now we can start running some analysis. Let's calculate some simple statistics of difference between gridded rainfall estimates and rain gauge observations.\n",
    "\n",
    "\n",
    "## 3.1 Join one gauge to one grid cell\n",
    "First, let's look at how we can join a single rain gauge to its nearest grid cell in the CEH-GEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_GAUGE_ID = 90358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nltQsaMbdaxy"
   },
   "outputs": [],
   "source": [
    "# filter the important metadata\n",
    "one_gauge = severn_rain_gauge_data_filtered.filter(pl.col(\"ID\") == CHOSEN_GAUGE_ID)\n",
    "one_gauge_metadata = severn_rain_gauge_metadata.filter(pl.col(\"ID\") == CHOSEN_GAUGE_ID)\n",
    "one_gauge_completeness = severn_rain_gauge_data_completeness.filter(\n",
    "    pl.col(\"ID\") == CHOSEN_GAUGE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_metadata  # ah, 90358 is SUGNALL HALL and it is at a height of 142 meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `method='nearest'` argument to get the nearest grid cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial subset (get nearest grid cell)\n",
    "one_gauge_grid_cell = severn_gear_daily.sel(\n",
    "    x=one_gauge_metadata[\"EASTING\"], y=one_gauge_metadata[\"NORTHING\"], method=\"nearest\"\n",
    ")\n",
    "\n",
    "# time subset (get data that overlaps in time)\n",
    "one_gauge_grid_cell = one_gauge_grid_cell.sel(\n",
    "    time=slice(\n",
    "        one_gauge_completeness[\"start_date\"].item(),\n",
    "        one_gauge_completeness[\"end_date\"].item(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both time series on the same y-axis\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9, 7), sharex=True, sharey=True)\n",
    "\n",
    "one_gauge.to_pandas().set_index(\"DATETIME\")[\"PRECIPITATION\"].plot(ax=axes[0])\n",
    "one_gauge_grid_cell[\"rainfall_amount\"].plot(ax=axes[1])\n",
    "\n",
    "axes[0].set_title(f\"Rain gauge (ID={CHOSEN_GAUGE_ID})\")\n",
    "axes[1].set_title(\"Closest CEH-GEAR grid-cell\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(\"Precipitation (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look a bit similar, but is hard to tell because this is a daily record.  \n",
    "Let's more directly compare the two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from the nearest grid cell\n",
    "one_gauge_grid_cell_df = pl.DataFrame(\n",
    "    {\n",
    "        \"DATETIME\": one_gauge_grid_cell[\"time\"].data.flatten(),\n",
    "        \"PRECIPITATION_GRID\": one_gauge_grid_cell[\"rainfall_amount\"].data.flatten(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# This is needed to get datetime in correct format\n",
    "one_gauge_grid_cell_df = one_gauge_grid_cell_df.with_columns(\n",
    "    pl.col(\"DATETIME\").cast(pl.Datetime(\"us\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data together\n",
    "one_gauge_joined = one_gauge.join(one_gauge_grid_cell_df, on=\"DATETIME\", how=\"left\")\n",
    "one_gauge_joined = one_gauge_joined.rename({\"PRECIPITATION\": \"PRECIPITATION_GAUGE\"})\n",
    "one_gauge_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Question: The two rainfall records seem similar, but are they correlated?\n",
    "*Hint: see cell below*  \n",
    "**Answer:** ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a pearson Correlation\n",
    "one_gauge_joined.select(\n",
    "    pl.corr(\"PRECIPITATION_GAUGE\", \"PRECIPITATION_GRID\", method=\"pearson\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute a difference through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_joined = one_gauge_joined.with_columns(\n",
    "    (pl.col(\"PRECIPITATION_GAUGE\") - pl.col(\"PRECIPITATION_GRID\")).alias(\n",
    "        \"rainfall_diff\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values so that we are not including values where either the CEH-GEAR or rain gauge is a missing value\n",
    "one_gauge_joined = one_gauge_joined.drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_joined.to_pandas().set_index(\"DATETIME\")[\"rainfall_diff\"].plot()\n",
    "plt.title(\n",
    "    f\"Difference between gauge {CHOSEN_GAUGE_ID} & the nearest CEH-GEAR grid cell\"\n",
    ")\n",
    "plt.ylabel(\"Rainfall difference (mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean difference =\", one_gauge_joined[\"rainfall_diff\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the variability of the rainfall difference might change through after around 2009, let's look quickly look at the standard deviation before and after 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_difference_before2000 = one_gauge_joined.filter(pl.col(\"DATETIME\") < pl.date(year=2009, month=1, day=1))['rainfall_diff']\n",
    "rainfall_difference_after2009 = one_gauge_joined.filter(pl.col(\"DATETIME\") >= pl.date(year=2009, month=1, day=1))['rainfall_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Standard deviation of difference 1990-2009: {rainfall_difference_before2000.std()}\")\n",
    "print(f\"Standard deviation in difference 2009-2019: {rainfall_difference_after2009.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also looks like there may be an decrease in the number of days where the estimate difference is above -/+ 1 mm\n",
    "#### 🤨 Optional Task: Use the `filter` method to get the number of days where the rainfall difference is -/+ 1 and then count the number of days each year\n",
    "\n",
    "Replace the ??? below with your answer\n",
    "\n",
    "*Hint: because you are looking for +/- 1, it is a good idea to use absolute numbers i.e. abs()*  \n",
    "*Hint: group_by_dynamic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Look at monthly running total\n",
    "To make the plot more visually appealing, we can compute the difference in rainfall based on monthly sum. this wasy, we will get less spikes \n",
    "\n",
    "TODO Running average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1750773305891,
     "user": {
      "displayName": "Tom Keel",
      "userId": "06399654286152299583"
     },
     "user_tz": -60
    },
    "id": "xtY9lb7AAvBW"
   },
   "outputs": [],
   "source": [
    "# group data into monthly sum\n",
    "one_gauge_monthly_sums = one_gauge_joined.group_by_dynamic(\"DATETIME\", every=\"1mo\").agg(\n",
    "    [pl.col(\"PRECIPITATION_GAUGE\").sum(), pl.col(\"PRECIPITATION_GRID\").sum()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_monthly_sums = one_gauge_monthly_sums.with_columns(\n",
    "    (pl.col(\"PRECIPITATION_GAUGE\") - pl.col(\"PRECIPITATION_GRID\")).alias(\n",
    "        \"rainfall_diff\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_monthly_sums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc0XNt7dAz5V"
   },
   "source": [
    "#### 🤨 Task: What is the mean monthly difference?\n",
    "Replace the ??? below with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_monthly_sums[\"rainfall_diff\"].???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_monthly_sums.to_pandas().set_index(\"DATETIME\")[\"rainfall_diff\"].plot()\n",
    "plt.axhline(color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc0XNt7dAz5V",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Hmm, is there any sort of trend in this data? Is it related to the number of gauges in the Upper Severn? That's a research question for another time...\n",
    "\n",
    "##### ❓ Question for now: Is the nearest CEH-GEAR grid cell over or underestimating rainfall relative to the rain gauge? \n",
    "\n",
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤨 Task: Change the resolution of the time group_by below to:\n",
    "1. 6 months\n",
    "2. 1 year\n",
    "3. Resolution of your chosing\n",
    "\n",
    "Replace the ??? below with your answer\n",
    "\n",
    "*Hint: find out online the correct syntax to group every 6 months and 1 year*\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gauge_joined.group_by_dynamic(\"DATETIME\", every=???).agg(\n",
    "    [\n",
    "    pl.col(\"PRECIPITATION_GAUGE\").sum(),\n",
    "    pl.col(\"PRECIPITATION_GRID\").sum()\n",
    "    ]\n",
    ")\n",
    "\n",
    "??? = ???.with_columns(\n",
    "    (pl.col(\"PRECIPITATION_GAUGE\") - pl.col(\"PRECIPITATION_GRID\")).alias(\"rainfall_diff\")\n",
    ")\n",
    "\n",
    "???.to_pandas().set_index(\"DATETIME\")['rainfall_diff'].plot()\n",
    "plt.axhline(color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc0XNt7dAz5V"
   },
   "source": [
    "#### 🤨 Task: Before moving on, go back to 3.1 and replace CHOSEN_GAUGE_ID with a new ID, then re-run all cells in 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Repeating for multiple rain gauges\n",
    "A lot of data research involves trial and error and repetition. It is always a good idea to get something working with a small subset before scaling up and applying your analysis to an entire dataset. In our case, we have code to compute a simple difference between a rain gauge and its nearest CEH-GEAR grid cell. Now let's scale that up to look at the entire catchment...\n",
    "\n",
    "> In this section, we bring everything together, and scale up our research code   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some functions with relatively descriptive names that will carry out all the analysis we saw in section 3.1.\n",
    "In writing these, we have tried to stick to the Single Responsibility Principle as best we can (that is, one function does one thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_diff_between_rain_gauge_and_nearest_grid_cell(gauge_id):\n",
    "    \"\"\"\n",
    "    Calculate mean difference between a given rain gauge and the nearest CEH-GEAR grid cell\n",
    "    \"\"\"\n",
    "    # 1. Load in gauge data and related metadata\n",
    "    one_gauge = severn_rain_gauge_data_filtered.filter(pl.col(\"ID\") == gauge_id)\n",
    "    one_gauge_metadata = severn_rain_gauge_metadata.filter(pl.col(\"ID\") == gauge_id)\n",
    "    one_gauge_completeness = severn_rain_gauge_data_completeness.filter(\n",
    "        pl.col(\"ID\") == gauge_id\n",
    "    )\n",
    "\n",
    "    # 2. Get nearest CEH-GEAR grid cell\n",
    "    one_gauge_grid_cell = get_nearest_gear_data_by_coords(\n",
    "        easting=one_gauge_metadata[\"EASTING\"], northing=one_gauge_metadata[\"NORTHING\"]\n",
    "    )\n",
    "\n",
    "    # 3. Subset nearest grid cell data by start end date of gauge\n",
    "    one_gauge_grid_cell = subset_gear_data_by_gauge_start_end_date(\n",
    "        gear_data=one_gauge_grid_cell,\n",
    "        start_date=one_gauge_completeness[\"start_date\"].item(),\n",
    "        end_date=one_gauge_completeness[\"end_date\"].item(),\n",
    "    )\n",
    "\n",
    "    # 4. Convert data into polars format\n",
    "    one_gauge_grid_cell_df = convert_gear_data_to_polars_df(one_gauge_grid_cell)\n",
    "\n",
    "    # 5. Join to rain gauge data\n",
    "    one_gauge_joined = one_gauge.join(one_gauge_grid_cell_df, on=\"DATETIME\", how=\"left\")\n",
    "    one_gauge_joined = one_gauge_joined.rename({\"PRECIPITATION\": \"PRECIPITATION_GAUGE\"})\n",
    "\n",
    "    # 6. Calculate daily rainfall differences\n",
    "    one_gauge_joined = calculate_rainfall_difference(one_gauge_joined)\n",
    "    return one_gauge_joined\n",
    "\n",
    "\n",
    "def get_nearest_gear_data_by_coords(easting, northing):\n",
    "    return severn_gear_daily.sel(x=easting, y=northing, method=\"nearest\")\n",
    "\n",
    "\n",
    "def subset_gear_data_by_gauge_start_end_date(gear_data, start_date, end_date):\n",
    "    return gear_data.sel(time=slice(start_date, end_date))\n",
    "\n",
    "\n",
    "def convert_gear_data_to_polars_df(gear_data):\n",
    "    gear_data_rainfall_amount = gear_data[\"rainfall_amount\"].data.flatten()\n",
    "    gear_data_time = gear_data[\"time\"].data.flatten()\n",
    "\n",
    "    gear_data_df = pl.DataFrame(\n",
    "        {\"DATETIME\": gear_data_time, \"PRECIPITATION_GRID\": gear_data_rainfall_amount}\n",
    "    )\n",
    "    gear_data_df = gear_data_df.with_columns(pl.col(\"DATETIME\").cast(pl.Datetime(\"us\")))\n",
    "    return gear_data_df\n",
    "\n",
    "\n",
    "def calculate_rainfall_difference(one_gauge_joined):\n",
    "    one_gauge_joined = one_gauge_joined.with_columns(\n",
    "        (pl.col(\"PRECIPITATION_GAUGE\") - pl.col(\"PRECIPITATION_GRID\")).alias(\n",
    "            \"rainfall_diff\"\n",
    "        )\n",
    "    )\n",
    "    one_gauge_joined = one_gauge_joined.drop_nulls()\n",
    "    return one_gauge_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_GAUGE_ID = 90358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_mean_diff_between_rain_gauge_and_nearest_grid_cell(gauge_id=CHOSEN_GAUGE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an easy one line to compute rainfall difference, we can loop through each gauge and compute some overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ids\n",
    "all_unique_gauge_ids = severn_rain_gauge_data_filtered[\"ID\"].unique().to_list()\n",
    "all_unique_gauge_ids[:5] + [\"...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Store all statistics in a dictionary\n",
    "all_gauge_comparisons = []\n",
    "\n",
    "for gauge_id in all_unique_gauge_ids:\n",
    "    gauge_comparison = {}\n",
    "    one_gauge_rainfall_diff = calc_mean_diff_between_rain_gauge_and_nearest_grid_cell(\n",
    "        gauge_id=gauge_id\n",
    "    )\n",
    "    gauge_comparison[\"gauge_id\"] = gauge_id\n",
    "    gauge_comparison[\"stdev_diff\"] = one_gauge_rainfall_diff[\"rainfall_diff\"].mean()\n",
    "    gauge_comparison[\"mean_diff\"] = one_gauge_rainfall_diff[\"rainfall_diff\"].std()\n",
    "    gauge_comparison[\"max_diff\"] = one_gauge_rainfall_diff[\"rainfall_diff\"].max()\n",
    "    gauge_comparison[\"min_diff\"] = one_gauge_rainfall_diff[\"rainfall_diff\"].min()\n",
    "    all_gauge_comparisons.append(gauge_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gauge_comparisons_df = pl.from_dicts(all_gauge_comparisons)\n",
    "all_gauge_comparisons_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of gauges, but a heatmap might be a quick visual way to check these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [\"min_diff\", \"mean_diff\", \"stdev_diff\", \"max_diff\"]\n",
    "fig, ax = plt.subplots(1, figsize=(5, 9))\n",
    "sns.heatmap(\n",
    "    all_gauge_comparisons_df.select(cols_to_plot),\n",
    "    cmap=\"RdBu_r\",\n",
    "    ax=ax,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cbar=False,\n",
    ")\n",
    "ax.set_xticklabels(cols_to_plot)\n",
    "ax.set_ylabel(\"Gauge number\")\n",
    "ax.set_title(\"Summary of rain gauge difference from CEH-GEAR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, mean and standard deviation are relatively simple summary statistics, but you can imagine we can run more advanced statistics just as easily in this structure.\n",
    "\n",
    "#### 🤨 Optional task: How correlated are each of the rain gauges to their nearest CEH-GEAR grid cell? \n",
    "If you cannot immediately think of how to do this, please move on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, Let's join this data back to the metadata so we can plot the gauges on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_metadata_w_comparison = severn_rain_gauge_metadata.join(\n",
    "    all_gauge_comparisons_df, left_on=\"ID\", right_on=\"gauge_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_metadata_w_comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6), sharex=True, sharey=True)\n",
    "ax.set_xlabel(\"Easting\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "sns.scatterplot(\n",
    "    x=\"EASTING\",\n",
    "    y=\"NORTHING\",\n",
    "    hue=\"stdev_diff\",\n",
    "    palette='RdBu_r',\n",
    "    data=severn_rain_gauge_metadata_w_comparison,\n",
    "    ax=ax,\n",
    ")\n",
    "abermule_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "bewdley_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "dolwen_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)\n",
    "plynlimon_shp.plot(ax=ax, facecolor=\"none\", alpha=0.8, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, there is not a clear spatial pattern in the difference between gauge and gridded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Compute catchment averages and compare gridded rainfall to rain gauge observation\n",
    "In this final example, we will compute catchment average rainfall for the smaller catchments in the Upper Severn (where the rain gauge network is more complete)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to make a mask of the smaller catchments. To do this, we have prepared a small function `make_region_hght_clip` that can clip the topography data, we can then use that to mask CEH-GEAR rainfall data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make region clips using the Upper Severn catchment shapefiles\n",
    "abermule_hght = data_utils.make_region_hght_clip(abermule_shp, hght_data=severn_hght)\n",
    "dolwen_hght = data_utils.make_region_hght_clip(dolwen_shp, hght_data=severn_hght)\n",
    "plynlimon_hght = data_utils.make_region_hght_clip(plynlimon_shp, hght_data=severn_hght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about understanding the code, but basically we can show what this is doing in the next plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "severn_hght.plot(ax=axes[0], vmax=8.8, alpha=0.5, add_colorbar=False)\n",
    "abermule_hght.plot(ax=axes[1], vmax=8.8, alpha=0.5, add_colorbar=False)\n",
    "\n",
    "for ax in axes:\n",
    "    abermule_shp.plot(ax=ax, facecolor=\"none\", edgecolor=\"C2\")\n",
    "    ax.set_title(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can mask the CEH-GEAR rainfall data to each region using another function `mask_regional_rainfall` which will create a subset based on the masks we just created. Again do not worry about the specifics, but if interested please look in the `data_utils.py` function. On a side note, it is always a good idea to move heavier functions outside of the notebook to keep things clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes 60 seconds\n",
    "abermule_mask_rainfall = data_utils.mask_region_rainfall(\n",
    "    severn_gear_daily, abermule_hght\n",
    ")\n",
    "dolwen_mask_rainfall = data_utils.mask_region_rainfall(severn_gear_daily, dolwen_hght)\n",
    "plynlimon_mask_rainfall = data_utils.mask_region_rainfall(\n",
    "    severn_gear_daily, plynlimon_hght\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this, so we can see what it has done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_examine = \"2018-01-01\"  # feel free to look at other dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "severn_gear_daily.sel(time=date_to_examine)[\"rainfall_amount\"].plot(\n",
    "    ax=axes[0], vmax=12, cmap=\"Blues\", cbar_kwargs={\"label\": \"Daily rainfall (mm)\"}\n",
    ")\n",
    "\n",
    "abermule_mask_rainfall.sel(time=date_to_examine)[\"rainfall_amount\"].plot(\n",
    "    ax=axes[1], vmax=12, cmap=\"Blues\", cbar_kwargs={\"label\": \"Daily rainfall (mm)\"}\n",
    ")\n",
    "\n",
    "for ax in axes:\n",
    "    abermule_shp.plot(ax=ax, facecolor=\"none\", edgecolor=\"C2\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(f\"Date: {date_to_examine}\", size=20)\n",
    "axes[0].set_title(\"Rainfall across Severn region\")\n",
    "axes[1].set_title(\"Rainfall in Abermule catchment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Calculate the 1990-2019 mean catchment and compare rainfall estimates across the 3 Upper Severn catchments \n",
    "Okay, we have the masks, now let's compute a 1990-2010 mean rainfall to see which parts of the catchment are wettest on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments_shp_and_hght = {\n",
    "    \"Abermule\": {\"shp\": abermule_shp, \"hght\": abermule_hght},\n",
    "    \"Dolwen\": {\"shp\": dolwen_shp, \"hght\": dolwen_hght},\n",
    "    \"Plynlimon Flume\": {\"shp\": plynlimon_shp, \"hght\": plynlimon_hght},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "abermule_rain = data_utils.mask_region_rainfall(\n",
    "    severn_gear_daily, catchments_shp_and_hght[\"Abermule\"][\"hght\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the spatial mean, we need to run `.mean(\"time\")`, so it will take a mean of the time coordinates (leaving only x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "abermule_mean_rain = abermule_rain.mean(\"time\")[\"rainfall_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "abermule_mean_rain.plot(ax=ax)\n",
    "catchments_shp_and_hght[\"Abermule\"][\"shp\"].plot(\n",
    "    ax=ax, facecolor=\"none\", edgecolor='C2', alpha=0.5\n",
    ")\n",
    "ax.set_xlim(280000, 330000)\n",
    "ax.set_ylim(270000, 310000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through each of the catchments and plot them next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "for ax, catchment, color_to_use in zip(\n",
    "    axes.flatten(), catchments_shp_and_hght.keys(), [\"C2\", \"C1\", \"C3\"]\n",
    "):\n",
    "    region_mask_rain = data_utils.mask_region_rainfall(\n",
    "        severn_gear_daily, catchments_shp_and_hght[catchment][\"hght\"]\n",
    "    ).mean(\"time\")[\"rainfall_amount\"]\n",
    "    region_mask_rain.plot(\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=9,\n",
    "        cmap=\"Blues\",\n",
    "        cbar_kwargs={\"label\": \"Daily rainfall (mm)\"},\n",
    "    )\n",
    "    catchments_shp_and_hght[catchment][\"shp\"].plot(\n",
    "        ax=ax, facecolor=\"none\", edgecolor=color_to_use, alpha=0.5\n",
    "    )\n",
    "    ax.set_title(f\"{catchment}\", size=16)\n",
    "    ax.set_xlabel(\"Easting\")\n",
    "    ax.set_ylabel(\"Northing\")\n",
    "    ax.text(\n",
    "        s=f\"mean={region_mask_rain.mean():.2f}\",\n",
    "        x=306000,\n",
    "        y=274000,\n",
    "        size=12,\n",
    "        color=color_to_use,\n",
    "        alpha=0.9,\n",
    "    )\n",
    "axes[1][1].remove()\n",
    "axes[1][0].text(\n",
    "    s=\"CEH-GEAR gridded rainfall\\n      (1990-2019 mean)\",\n",
    "    x=345000,\n",
    "    y=290000,\n",
    "    size=18,\n",
    "    color=\"grey\",\n",
    ")\n",
    "ax.set_xlim(280000, 330000)\n",
    "ax.set_ylim(270000, 310000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if the height profile has anything to do with spatial pattern of the mean rainfall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Comparing catchment rainfall estimates through time\n",
    "In 3.3.1, we look at the spatial catchment average, how about the time-average of the catchment rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep RAM-use low, let's take a subset of data from 2010-2019\n",
    "abermule_mask_rainfall_2010s = abermule_mask_rainfall.sel(time=slice(\"2010\", \"2019\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the time mean, we need to run `.mean((\"x\", \"y\"))`, so it will take a mean of the x, y coordinates (leaving only time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "abermule_catchment_mean_rainfall_2010s = abermule_mask_rainfall_2010s[\n",
    "    \"rainfall_amount\"\n",
    "].mean((\"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_catchment_mean_rainfall_2010s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, what about the rain gauges within those catchments, what do they estimate?  \n",
    "Next, we will compare the catchment average of the CEH-GEAR to the average of the rain gauges in the catchment. For this purpose, we will convert the metadata into a geodataframe. This will allow us to compute a [Point in Polygon](https://en.wikipedia.org/wiki/Point_in_polygon) operation, where the 'points' are the rain gauge location, and the 'polygons' are the catchment boundaries i.e. `abermule_shp` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a geometry column for the metadata gdf based on the X,Y coordiantes of rain gauges\n",
    "geometry = [\n",
    "    shapely.geometry.Point(xy)\n",
    "    for xy in zip(\n",
    "        severn_rain_gauge_metadata_w_comparison[\"EASTING\"],\n",
    "        severn_rain_gauge_metadata_w_comparison[\"NORTHING\"],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame using that geometry\n",
    "severn_rain_gauge_metadata_gdf = gpd.GeoDataFrame(\n",
    "    severn_rain_gauge_metadata_w_comparison.to_pandas(),\n",
    "    geometry=geometry,\n",
    "    crs=\"EPSG:27700\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_metadata_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Question: What data type is `severn_rain_gauge_metadata_gdf`?\n",
    "**Answer:** ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data we just created using a web map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_metadata_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use a `.contains()` method to compute Point in Polygon operation i.e. is a given rain gauge within the abermule shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_metadata = severn_rain_gauge_metadata_gdf.loc[\n",
    "    severn_rain_gauge_metadata_gdf.apply(\n",
    "        lambda row: abermule_shp.contains(row[\"geometry\"]), axis=1\n",
    "    ).values\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the gauges within one of the Upper Severn catchments: Abermule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "abermule_metadata.plot(ax=ax)\n",
    "abermule_shp.plot(ax=ax, facecolor=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the rain gauge data for those Abermule gauge IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data = severn_rain_gauge_data_filtered.filter(\n",
    "    pl.col(\"ID\").is_in(abermule_metadata[\"ID\"].to_list())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we pivot the Abermule rain gauge data so that each column refers to a different gauge. Rather than describing how this works, have a look at the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_pivot = abermule_rain_gauge_data.pivot(\n",
    "    on=\"ID\", index=\"DATETIME\", values=\"PRECIPITATION\"\n",
    ")\n",
    "\n",
    "# Sort the pivot table by time (for plotting purporses)\n",
    "abermule_rain_gauge_data_pivot = abermule_rain_gauge_data_pivot.sort(by=\"DATETIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quickly visualise the 6 month rolling mean for these Abermule rain gauges. Don't worry about the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_6month = abermule_rain_gauge_data_pivot.group_by_dynamic(\n",
    "    \"DATETIME\", every=\"6mo\"\n",
    ").agg([pl.col(col).mean() for col in abermule_rain_gauge_data_pivot.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_6month.to_pandas().set_index(\"DATETIME\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will get data from 2010-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_2010s = abermule_rain_gauge_data_pivot.filter(\n",
    "    (pl.col(\"DATETIME\") >= pl.datetime(year=2010, month=1, day=1))\n",
    "    & (pl.col(\"DATETIME\") < pl.datetime(year=2020, month=1, day=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute a mean of all the gauges in the Abermule catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_2010s = abermule_rain_gauge_data_2010s.with_columns(\n",
    "    pl.mean_horizontal(pl.all().exclude(\"DATETIME\")).alias(\"gauge mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_2010s[\"gauge mean\"].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the catchment-wide mean from CEH-GEAR again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_catchment_mean_rainfall_2010s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm, they look similar, but are they? See additional task 1 if you would like to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving research outputs\n",
    "Finally, it is essential when doing any research to save your outputs. Especially when the outputs take a while to load, and compute.   \n",
    "Because we have read `.csv` files with polars, we can use polars to save `.csv` files.  \n",
    "Because we have read `.nc` (NetCDF) files with xarray, we can use polars to save `.nc` files.  \n",
    "\n",
    "We'll give you some examples below...\n",
    "\n",
    "#### 🤨 Optional task: Save research outputs to appropriate file system (and to use later) \n",
    "Replace the `???` with the path to where you want to save your data, please ask for help if needed\n",
    "\n",
    "*NOTE: you might save these things under different names*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_your_directory = ???\n",
    "abermule_rain_gauge_data_2010s.write_csv(f\"{path_to_your_directory}/abermule_rain_gauge_data_2010s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_your_directory = ???\n",
    "abermule_catchment_mean_rainfall_2010s.to_file(f\"{path_to_your_directory}/abermule_catchment_mean_rainfall_2010s.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfxqKKmYZKGE"
   },
   "source": [
    "## ❗❗ Additional tasks ❗❗  \n",
    "\n",
    "Feel free to stop at this point, but below are some additional and more advanced topics and tasks requiring more of your own input. We will provide help.  \n",
    "\n",
    "Select any of the following:\n",
    "\n",
    "*Task 1. Compare catchment-wide rainfall estimates (grid vs gauge)* 🟢  \n",
    "*Task 2. Does rainfall disproportionately fall at higher altitudes before floods?* 🔴  \n",
    "*Task 3. A case study of an unseen rain gauge in the Upper Severn* 🟡   \n",
    "*Task 4. Looking at future projections of precipitation in the Severn using CHESS-SCAPE* 🔴\n",
    "\n",
    "#### Task Difficulty:\n",
    "- 🟢 -> easy, you should have all of the code to do this\n",
    "- 🟡 -> moderate, will require a little more work and a better understanding of the data\n",
    "- 🔴 -> hard, requires lots of independent thought\n",
    "\n",
    "\n",
    "## > ⭐ Please do any work for the additional tasks in a new notebook, save any useful research outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🟢 Additonal Task 1: Compare catchment-mean rainfall estimates CEH-GEAR vs Rain Gauge Observation\n",
    "> Note: please do this in a new notebook file\n",
    "\n",
    "We already should have catchment-mean rainfall for the Abermule catchment loaded in (in section 3.3.2), so you choose to do this task just below. It may be a good learning opportunity to develop your own statistical analysis. Perhaps by running a linear regression?\n",
    "\n",
    "*Hint: you could import the `scipy.stats` library to run statistics and `numpy` gives you access to regression*  \n",
    "*Hint: Consider using (monthly) running means to plot data*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_catchment_mean_rainfall_2010s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abermule_rain_gauge_data_2010s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔴 Additional Task 2. Does rainfall disproportionately fall at higher altitudes during floods?\n",
    "> Note: please do this in a new notebook file\n",
    "\n",
    "If you need a hint, check out some of the plots and notebooks in: https://github.com/NERC-CEH/FDRI-high-altitude-rainfall-and-floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that the heaviest rainfall is often a day or two before the flood event\n",
    "EXAMPLE_FLOOD_DAYS_IN_SEVERN = [\n",
    "    \"1984-11-13\",\n",
    "    \"1986-12-16\",\n",
    "    \"1986-12-19\",\n",
    "    \"1988-03-16\",\n",
    "    \"1988-03-20\",\n",
    "    \"1990-03-01\",\n",
    "    \"1990-12-27\",\n",
    "    \"1994-01-04\",\n",
    "    \"1994-01-14\",\n",
    "    \"1994-02-28\",\n",
    "    \"1994-04-05\",\n",
    "    \"1994-04-10\",\n",
    "    \"1995-01-22\",\n",
    "    \"1995-12-24\",\n",
    "    \"1996-02-13\",\n",
    "    \"2000-09-28\",\n",
    "    \"2001-02-13\",\n",
    "    \"2002-01-28\",\n",
    "    \"2002-11-15\",\n",
    "    \"2007-01-01\",\n",
    "    \"2008-03-17\",\n",
    "    \"2008-11-11\",\n",
    "    \"2012-04-30\",\n",
    "    \"2013-02-15\",\n",
    "    \"2020-01-15\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for masking rainfall data by height\n",
    "def mask_region_rainfall_by_hght(rainfall_data, region_hght, threshold):\n",
    "    region_hght_mask = region_hght / region_hght.where(region_hght > threshold)\n",
    "    return rainfall_data * region_hght_mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0ooibSwTPlP"
   },
   "source": [
    "## 🟡 Additional Task 3. Case Study: Examining unseen rain gauge data in the Upper Severn\n",
    "> Note: please do this in a new notebook file\n",
    "\n",
    "The CEH-GEAR gridded product was produced on the rain gauge data we have used in this practical. As such, there is a direct spill of information between them. On the JASMIN object store there is an additional \"unseen\" rain gauge near Carreg Wen within the Plynlimon catchment. \n",
    "\n",
    "#### Some research questions:\n",
    "- How closely do the CEH grid cells around Carreg Wen estimate rainfall?\n",
    "- Has the gridded product under or over-estimated rainfall in the days before a flood?\n",
    "\n",
    "*Hint: If you really need a hint, you can check this [notebook](https://github.com/NERC-CEH/FDRI-comparing-rainfall-data-in-upper-severn/blob/main/notebooks/Carreg_wen_case_study.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carreg_wen_daily = pl.read_csv(\n",
    "    \"s3://rain-gauge/carreg_wen_daily_rainfall.csv\",\n",
    "    storage_options={\"endpoint_url\": \"https://fdri-o.s3-ext.jc.rl.ac.uk\", \"anon\": True},\n",
    "    try_parse_dates=True,\n",
    ")\n",
    "carreg_wen_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severn_rain_gauge_metadata.filter(pl.col(\"NAME\") == \"CARREG WEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "abermule_metadata.plot(ax=ax)\n",
    "abermule_shp.plot(ax=ax, facecolor=\"none\")\n",
    "plynlimon_shp.plot(ax=ax, facecolor=\"none\")\n",
    "ax.scatter(x=282900, y=288500)\n",
    "ax.text(s=\"Carreg Wen\", x=284700, y=288200, c=\"C1\")\n",
    "ax.legend([\"Gauges used in CEH-GEAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that the heaviest rainfall is often a day or two before the flood event\n",
    "EXAMPLE_FLOOD_DAYS_IN_SEVERN = [\n",
    "    \"1984-11-13\",\n",
    "    \"1986-12-16\",\n",
    "    \"1986-12-19\",\n",
    "    \"1988-03-16\",\n",
    "    \"1988-03-20\",\n",
    "    \"1990-03-01\",\n",
    "    \"1990-12-27\",\n",
    "    \"1994-01-04\",\n",
    "    \"1994-01-14\",\n",
    "    \"1994-02-28\",\n",
    "    \"1994-04-05\",\n",
    "    \"1994-04-10\",\n",
    "    \"1995-01-22\",\n",
    "    \"1995-12-24\",\n",
    "    \"1996-02-13\",\n",
    "    \"2000-09-28\",\n",
    "    \"2001-02-13\",\n",
    "    \"2002-01-28\",\n",
    "    \"2002-11-15\",\n",
    "    \"2007-01-01\",\n",
    "    \"2008-03-17\",\n",
    "    \"2008-11-11\",\n",
    "    \"2012-04-30\",\n",
    "    \"2013-02-15\",\n",
    "    \"2020-01-15\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjuHdOfVZMMQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYJqszrFb4he"
   },
   "source": [
    "## 🔴 Additional Task 4. Looking at future projections of precipitation using CHESS-SCAPE\n",
    "> Note: please do this in a new notebook file\n",
    "\n",
    "CHESS-SCAPE provides future projections of precipitation across the UK. This rich dataset enables a wide range of climate-related research questions. You can explore and analyze these data using code similar to what's shown in this notebook. \n",
    "\n",
    "#### Example research questions\n",
    "- How is precipitation projected to change across the Upper Severn over time?\n",
    "- Which catchments are expected to experience the greatest increase in extreme rainfall events?\n",
    "\n",
    "> If you have a problem loading in the zarr file, please check you are using zarr>=3.0.8 and you have cftime installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezbXm2jsb1KP"
   },
   "outputs": [],
   "source": [
    "# We are accessing TASMAX & PRCPT for the Ensemble member #01 from the catalogue\n",
    "fs = fsspec.filesystem(\n",
    "    \"s3\",\n",
    "    asynchronous=True,\n",
    "    anon=True,\n",
    "    endpoint_url=\"https://chess-scape-o.s3-ext.jc.rl.ac.uk\",\n",
    ")\n",
    "pr_zstore = zarr.storage.FsspecStore(\n",
    "    fs, path=\"ens01-year100kmchunk/pr_01_year100km.zarr\"\n",
    ")\n",
    "\n",
    "chess_pr = xr.open_zarr(\n",
    "    pr_zstore, decode_times=True, decode_cf=True, consolidated=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK9skQheRNf9"
   },
   "source": [
    "# Additional Reading\n",
    "\n",
    "- <https://github.com/NERC-CEH/FDRI-comparing-rainfall-data-in-upper-severn/tree/main>  \n",
    "\n",
    "- <https://github.com/NERC-CEH/FDRI-high-altitude-rainfall-and-floods>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyODr7BC4eqGQA5//dZiuhpK",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ukceh_ss_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
